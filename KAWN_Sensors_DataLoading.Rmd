---
title: 'KAWN: Data loading and cleaning'
author: "Michelle Catherine Kelly"
date: "3 June 2018"
output:
  html_document
---

```{r setup, echo = FALSE, warning=FALSE, error=FALSE, message=FALSE}
knitr::opts_chunk$set(cache=TRUE)

library(XML)
library(lubridate)
library(data.table)
library(dplyr)
library(tidyr)
library(dataRetrieval) # USGS loading
# NEON loading
# library(devtools)
# install_github("NEONScience/NEON-utilities/neonUtilities", dependencies = TRUE)
library(neonUtilities)
```

```{r dataLoad}
# masterFile
  # concatenates local nitratax, miniDOT files into a single master .csv (if no master exists). If data is located on USGS database, pulls from USGS database and saves local master. If local master is present, script loads from file
  # sitename = character vector with proper capitalization
  # USGS = logical vector, if TRUE data should is pulled from USGS, if FALSE data is loaded from local files

masterFile <- function(sitename, USGS){
  
  if(file.exists(paste("KAWN_SensorData_", sitename, ".csv", sep = ""))){
    # if master file exists, load from master file
    dataframe <- read.csv(paste("KAWN_SensorData_", sitename, ".csv", sep = ""), 
                          header = TRUE)
  }
  
  if(!file.exists(paste("KAWN_SensorData_", sitename, ".csv", sep = ""))){ 
    # if master file doesn't exist, compile master file
    if(USGS == FALSE){ 
      # load in nitratax data
      nitratax_XMLtoCSV <- function(filename){
        xmlData <- XML::xmlParse(file = filename)
        xmlData <- XML::xmlToDataFrame(xmlData, homogeneous = F, stringsAsFactors = F)
        xmlData <- xmlData[-(1:6),] # taking out text at top of dataframe
        for (count in 1:length(xmlData)){
          names(xmlData)[count] <- as.character(xmlData[1,count])
        }
        xmlData <- xmlData[-1,] # taking out character rows
        xmlData <- xmlData[,-1] # taking out first blank column
        xmlData$`  TIME  ` <- lubridate::as_datetime(xmlData$`  TIME  `, 
                                                     tz = "America/Chicago",
                                                     format = "%m/%d/%Y %H:%M:%S")
        xmlData$`  NITRATE CONC.  ` <- as.numeric(xmlData$`  NITRATE CONC.  `)
        xmlData$`  ER  ` <- as.numeric(xmlData$`  ER  `)
        xmlData$`  EM  ` <- as.numeric(xmlData$`  EM  `)
        
        xmlData <- data.frame(xmlData$`  TIME  `, xmlData$`  NITRATE CONC.  `)
        names(xmlData) <- c("dateTime", "nitrateNitrite")
        return(xmlData)
      }
      
      nitratax <- nitratax_XMLtoCSV(paste("NITRATAX_", sitename, ".xml", sep = ""))
      
      if(file.exists(paste("miniDOT_", sitename, ".txt", sep = ""))){
      # load in minidot data
        DO <- read.table(file = paste("miniDOT_", sitename, ".txt", sep = ""), 
                         header = TRUE, sep = ",", skip = 8, stringsAsFactors = FALSE, 
                         colClasses = c("numeric", "POSIXct", "POSIXct", "numeric", 
                                        "numeric", "numeric", "numeric", "numeric"),
                         col.names = c("unixTimestamp", "time_UTC", "time_CST", 
                                       "battery_V", "temp_C", "DO_mgL", "DO_sat", "Q"))
        DO <- DO[c(-1, -2, -4, -8)] #drop unix timestamp, time UTC, battery volts, Q
        DO$time_CST <- round_date(DO$time_CST, unit = "5 mins") # round time to nearest 5 min
        
        # merge minidot with nitratax by datetime
        dataframe <- merge(nitratax, DO, by.x = "dateTime", by.y = "time_CST", all = TRUE)
        # save master file
        write.csv(dataframe, row.names = FALSE,
                  file = paste("KAWN_SensorData_", sitename, ".csv", sep = ""))
        
        } else{
          # save master file with just nitratax
          dataframe <- nitratax
          write.csv(dataframe, row.names = FALSE,
                    file = paste("KAWN_SensorData_", sitename, ".csv", sep = ""))
          }
      } else{ # use USGS package to pull data from web, then save locally
        start <- ymd_hms("2018-01-01 00:00:00")
        end <- ymd_hms("2018-06-15 00:00:00")
        
        if(sitename == "Desoto"){
          siteNo <- "06892350"
          pCodes <- c("99133", "00060", "00065", "00300", "00301","00010", 
                      "00095", "00400", "32295")
          data <- readNWISuv(siteNumbers = siteNo, parameterCd = pCodes, 
                           startDate = start, endDate = end, tz = "America/Chicago")
        
          # Rename columns from codes to readable names
          data <- renameNWISColumns(data)
          # Fix remaining names that are still parameter codes
          names(data)[names(data)=="00301_Inst"] <- "DO_percentsat" 
          names(data)[names(data)=="99133_Inst"] <- "nitrateNitrite"
          
          # Convert from American to SI units
          # cubic foot per second to m3 per second
          ft3s_m3s <- function(ft3s){
            m3s <- ft3s*0.0283168
            return(m3s)
          }
          # foot to meters
          ft_m <- function(ft){
            m <- 0.3048*ft
            return(m)
          }
          data$Flow_Inst <- ft3s_m3s(data$Flow_Inst)
          data$GH_Inst <- ft_m(data$GH_Inst)
          
          # drop agency code, siteNo, various quality codes
          dataframe <- data[c(-1,-2,-5,-7,-9,-11,-13,-15,-17,-19,-21,-22)]
          
          # add column for UTC time
          dataframe$dateTime_UTC <- dataframe$dateTime + hours(5)
        }
        # save the USGS data locally
        write.csv(dataframe, row.names = FALSE,
                  file = paste("KAWN_SensorData_", sitename, ".csv", sep = ""))
      }
  }
  return(dataframe)
  }

bowersock <- masterFile(sitename = "Bowersock", USGS = FALSE)
eric <- masterFile(sitename = "Eric", USGS = FALSE)
steve <- masterFile(sitename = "Steve", USGS = FALSE)
desoto <- masterFile(sitename = "Desoto", USGS = TRUE)
```

```{r reachQ}
#if(Q.avg = TRUE && Qdata.avg file exists){
  #loadfile
#}
#if(Q.avg = FALSE && Qdata file exists){
  #load file
#}
#if(!Qdata.avg file exists | !Qdata file exists){
  #if (Q.avg = TRUE){
    #create Q.avg file
    #save Q.avg
  #}
  #if(Q.avg = FALSE){
    #create Q file
    #save Q file
  #}
#}

# avg.Q: logical, TRUE returns dataframe of mean daily Q at all sites, FALSE returns all measurements of Q at all sites
# load from file: logical, if TRUE loads appropriate dataframe from file
Q.dataframe <- function(avg.Q){
  if(avg.Q == TRUE & file.exists("KAWN_USGSDischarge_dailymeans.csv")){
    Qdata <- read.csv("KAWN_USGSDischarge_dailymeans.csv", header = TRUE)
  }
  
  if(avg.Q == FALSE & file.exists("KAWN_USGSDischarge_instantaneous.csv")){
    Qdata <- read.csv("KAWN_USGSDischarge_instantaneous.csv", header = TRUE)
  }
  
  if(!file.exists("KAWN_USGSDischarge_dailymeans.csv") |
     !file.exists("KAWN_USGSDischarge_instantaneous.csv")){
    Q1.lawrence <- "06891080" # USGS identifier codes for gauges of interest
    Q3.wakarusa <- "06891500"
    Q4.stranger <- "06892000"
    Q5.desoto <- "06892350"
    sites <- c(Q1.lawrence, Q3.wakarusa, Q4.stranger, Q5.desoto)
    start <- ymd_hms("2018-01-01 00:00:00") # start date of data
    end <- ymd_hms("2018-06-15 00:00:00") # end date of data
    
    #pull from USGS server
    Qdata <- readNWISuv(siteNumbers = sites, parameterCd = "00060", # discharge 
                          startDate = start, endDate = end, tz = "America/Chicago")
    Qdata <- renameNWISColumns(Qdata)
    
    ft3s_m3s <- function(ft3s){
      m3s <- ft3s*0.0283168
    return(m3s)
      }
    Qdata$Flow_Inst <- ft3s_m3s(Qdata$Flow_Inst) # convert to SI units
    Qdata$dateTime <- ymd_hms(Qdata$dateTime) # report date in lubridate format
    Qdata$Flow_Inst <- as.numeric(Qdata$Flow_Inst)
    Qdata <- Qdata[c(-1,-5,-6)] # take out the columns that report data quality codes
    
    if(avg.Q == TRUE){
      Qdata <- # compute an average value for each day
        Qdata %>% 
        group_by(site_no, dateTime = floor_date(dateTime, "day")) %>%
          summarize(Flow_Inst = mean(Flow_Inst))
    }
    
    Qdata <- spread(Qdata, site_no, Flow_Inst) # each site is in its own column
    names(Qdata)[names(Qdata)=="06891080"] <- "Q1.lawrence" 
    names(Qdata)[names(Qdata)=="06891500"] <- "Q3.wakarusa" 
    names(Qdata)[names(Qdata)=="06892000"] <- "Q4.stranger" 
    names(Qdata)[names(Qdata)=="06892350"] <- "Q5.desoto" 
    
    # read in flow data from farmland report
    Q2.Farmland <- read.csv("FarmlandDailyReport_MCKformatted.csv", 
                            header = TRUE, stringsAsFactors = FALSE)
    Q2.Farmland <- Q2.Farmland[c(-2:-8, -10:-11)]
    Q2.Farmland$Date <- as.POSIXct(Q2.Farmland$Date, format = "%m/%d/%y")
    Q2.Farmland <- 
      Q2.Farmland %>%
      group_by(Date = floor_date(Date, "day")) %>%
      summarise(flowRate_Outfall001A_gpm = mean(flowRate_Outfall001A_gpm))
    Q2.Farmland <- Q2.Farmland[Q2.Farmland$Date >= ymd("2018-01-01"),] # Jan 1 onwards
    
    # convert from gpm to m3s
    gpm_m3s <- function(gpm){
                m3s <- gpm*0.000063090196
                return(m3s)
    }
    Q2.Farmland$flowRate_Outfall001A_gpm <- gpm_m3s(Q2.Farmland$flowRate_Outfall001A_gpm)
    
    # merge Q2 with rest of Qdata
    Qdata <- merge(Qdata, Q2.Farmland, by.x = "dateTime", by.y = "Date", all = TRUE)
    names(Qdata)[names(Qdata)=="flowRate_Outfall001A_gpm"] <- "Q2.Farmland"
    
    # pumping stops on apr 1
    Qdata$Q2.Farmland[Qdata$dateTime > ymd("2018-04-01")] <- NA
    
    # balance equation: Q5.desoto = Q1.lawrence + Q2.farmland + Q3.wakarusa + Q4.stranger
    Qdata$Q5.desoto <- -1*Qdata$Q5.desoto
    Qdata$difference <- rowSums(Qdata[2:5], na.rm = TRUE) #difference = unaccounted for water, m3/s
    Qdata <- Qdata[Qdata$difference != 0.00,]# if row sum = 0, drop row
    
    if(avg.Q == "TRUE"){
      write.csv(Qdata, "KAWN_USGSDischarge_dailymeans.csv", row.names = FALSE)
    } else{
      write.csv(Qdata, "KAWN_USGSDischarge_instantaneous.csv", row.names = FALSE)
    }
  }
  return(Qdata)
  }
    
Qdata.avg <- Q.dataframe(avg.Q = TRUE)
Qdata.instant <- Q.dataframe(avg.Q = FALSE) # comb through this - a lot of na's throwing off the numbers

#plot(y = Qdata.avg$Q5.desoto, x = Qdata.avg$upstreamSum, main = "Sum upstream Q vs downstream Q")
#lm.Q <- summary(lm(Q5.desoto ~ upstreamSum, data = Qdata.avg))

#plot(x = Qdata.avg$day, y = Qdata.avg$waterBal_percentDev, main = "Percent error of water balance")
```
percent deviation = ((downstream Q - upstream Q) / upstream Q) * 100%  
where difference = sum (Lawrence gauge, Farmland flow, Wakarusa R. gage, Stranger Creek gage) - Desoto gage

positive values: upstream has greater Q  

negative values: downstream (desoto gauge) has greater Q  
- if precipitation was entering reach, this will happen  
- gages on wakarusa and stranger are pretty far upstream, unaccounting for any gains below the gage (therefore, water balance will tend to predict positive percent deviations)  


```{r dischargeEstimate}
# approximate discharge at site 2
# Q.site2 <- Q1.lawrence + Q2.farmland
#Qdata$Q1.lawrence + Qdata$
# Q.site2 <- Qdata[c(1,)]

# approximate discharge at site 3
# Q.site3 <- Q1.lawrence + Q2.farmland

```


```{r NEONpullCleanup, include=FALSE, eval=FALSE}
downloadNEONfiles <- FALSE

if (downloadNEONfiles == TRUE){
  dir.create(paste0(getwd(), "/NEONfiles"))
  
  ######## PAR ############
  getPackage(dpID = "DP1.00024.001", site_code = "UKFS",
           year_month = "2018-02", package = "basic",
           savepath = paste(getwd(), "NEONfiles", sep = "/")) # Feb at KU field station
  getPackage(dpID = "DP1.00024.001", site_code = "UKFS",
           year_month = "2018-03", package = "basic",
           savepath = paste(getwd(), "NEONfiles", sep = "/")) # March at KU field station
  #getPackage(dpID = "DP1.00024.001", site_code = "UKFS",
           #year_month = "2018-04", package = "basic",
           #savepath = getwd()) # April at KU field station 
  stackByTable(dpID = "DP1.00024.001", 
               filepath = paste0(getwd(), "/NEONfiles"), 
               savepath = paste0(getwd(), "/NEONfiles"), 
               folder = TRUE)
  
  ######## Barometric pressure ############
  getPackage(dpID = "DP1.00004.001", site_code = "UKFS",
           year_month = "2018-02", package = "basic",
           savepath = paste(getwd(), "NEONfiles", sep = "/")) # Feb at KU field station
  getPackage(dpID = "DP1.00004.001", site_code = "UKFS",
           year_month = "2018-03", package = "basic",
           savepath = paste(getwd(), "NEONfiles", sep = "/")) # March
  #getPackage(dpID = "DP1.00004.001", site_code = "UKFS",
           #year_month = "2018-04", package = "basic",
           #savepath = paste(getwd(), "NEONfiles", sep = "/")) # April
  stackByTable(dpID = "DP1.00004.001", 
               filepath = paste0(getwd(), "/NEONfiles"), 
               savepath = paste0(getwd(), "/NEONfiles"), 
               folder = TRUE)
}

PAR <- read.csv(file = paste0(getwd(), "/NEONfiles", "/stackedFiles", 
                              "/PARPAR_1min.csv"), header = TRUE)
PAR <- subset(PAR, subset = PAR$verticalPosition == 10) # take measurements that are closest to ground
PAR$startDateTime <- ymd_hms(PAR$startDateTime, tz = "America/Chicago")
PAR <- data.frame(
    dateTime = PAR$startDateTime,
    mean = PAR$PARMean,
    min = PAR$PARMinimum,
    max = PAR$PARMaximum,
    var = PAR$PARVariance
)

# PAR units are micromoles per m^2 per second

#### Barometric pressure, units are kPa
BarPress <- read.csv(file = paste0(getwd(), "/NEONfiles", "/stackedFiles", 
                              "/BP_30min.csv"), header = TRUE)
BarPress$startDateTime <- ymd_hms(BarPress$startDateTime, tz = "America/Chicago")
BarPress <- data.frame(
  dateTime = BarPress$startDateTime,
  mean = BarPress$staPresMean,
  min = BarPress$staPresMinimum,
  max = BarPress$staPresMaximum,
  var = BarPress$staPresVariance
)
# convert to mmHg
kpa_mmhg <- function(kpa){
  mmhg <- kpa*7.5006156130264
  return(mmhg)
}
BarPress$mean <- kpa_mmhg(BarPress$mean)
BarPress$min <- kpa_mmhg(BarPress$min)
BarPress$max <- kpa_mmhg(BarPress$max)
BarPress$var <- kpa_mmhg(BarPress$var)
```
