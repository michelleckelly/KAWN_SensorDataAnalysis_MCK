---
title: 'KAWN: Data loading and cleaning'
author: "Michelle Catherine Kelly"
date: "3 June 2018"
output:
  html_document
---

```{r setup, echo = FALSE, warning=FALSE, error=FALSE, message=FALSE}
knitr::opts_chunk$set(cache=TRUE)

library(XML)
library(lubridate)
library(data.table)
library(dplyr)
library(tidyr)
library(dataRetrieval) # USGS loading
# NEON loading
# library(devtools)
# install_github("NEONScience/NEON-utilities/neonUtilities", dependencies = TRUE)
library(neonUtilities)
```

```{r dataLoad}
# masterFile
  # concatenates local nitratax, miniDOT files into a single master .csv (if no master exists). If data is located on USGS database, pulls from USGS database and saves local master. If local master is present, script loads from file
  # sitename = character vector with proper capitalization
  # USGS = logical vector, if TRUE data should is pulled from USGS, if FALSE data is loaded from local files

masterFile <- function(sitename, USGS){
  
  if(file.exists(paste("KAWN_SensorData_", sitename, ".csv", sep = ""))){
    # if master file exists, load from master file
    dataframe <- read.csv(paste("KAWN_SensorData_", sitename, ".csv", sep = ""), 
                          header = TRUE)
  }
  
  if(!file.exists(paste("KAWN_SensorData_", sitename, ".csv", sep = ""))){ 
    # if master file doesn't exist, compile master file
    if(USGS == FALSE){ 
      # load in nitratax data
      nitratax_XMLtoCSV <- function(filename){
        xmlData <- XML::xmlParse(file = filename)
        xmlData <- XML::xmlToDataFrame(xmlData, homogeneous = F, stringsAsFactors = F)
        xmlData <- xmlData[-(1:6),] # taking out text at top of dataframe
        for (count in 1:length(xmlData)){
          names(xmlData)[count] <- as.character(xmlData[1,count])
        }
        xmlData <- xmlData[-1,] # taking out character rows
        xmlData <- xmlData[,-1] # taking out first blank column
        xmlData$`  TIME  ` <- lubridate::as_datetime(xmlData$`  TIME  `, 
                                                     tz = "America/Chicago",
                                                     format = "%m/%d/%Y %H:%M:%S")
        xmlData$`  NITRATE CONC.  ` <- as.numeric(xmlData$`  NITRATE CONC.  `)
        xmlData$`  ER  ` <- as.numeric(xmlData$`  ER  `)
        xmlData$`  EM  ` <- as.numeric(xmlData$`  EM  `)
        
        xmlData <- data.frame(xmlData$`  TIME  `, xmlData$`  NITRATE CONC.  `)
        names(xmlData) <- c("dateTime", "nitrateNitrite")
        return(xmlData)
      }
      
      nitratax <- nitratax_XMLtoCSV(paste("NITRATAX_", sitename, ".xml", sep = ""))
      
      if(file.exists(paste("miniDOT_", sitename, ".txt", sep = ""))){
      # load in minidot data
        DO <- read.table(file = paste("miniDOT_", sitename, ".txt", sep = ""), 
                         header = TRUE, sep = ",", skip = 8, stringsAsFactors = FALSE, 
                         colClasses = c("numeric", "POSIXct", "POSIXct", "numeric", 
                                        "numeric", "numeric", "numeric", "numeric"),
                         col.names = c("unixTimestamp", "time_UTC", "time_CST", 
                                       "battery_V", "temp_C", "DO_mgL", "DO_sat", "Q"))
        DO <- DO[c(-1, -2, -4, -8)] #drop unix timestamp, time UTC, battery volts, Q
        DO$time_CST <- round_date(DO$time_CST, unit = "5 mins") # round time to nearest 5 min
        
        # merge minidot with nitratax by datetime
        dataframe <- merge(nitratax, DO, by.x = "dateTime", by.y = "time_CST", all = TRUE)
        # save master file
        write.csv(dataframe, row.names = FALSE,
                  file = paste("KAWN_SensorData_", sitename, ".csv", sep = ""))
        
        } else{
          # save master file with just nitratax
          dataframe <- nitratax
          write.csv(dataframe, row.names = FALSE,
                    file = paste("KAWN_SensorData_", sitename, ".csv", sep = ""))
          }
      } else{ # use USGS package to pull data from web, then save locally
        start <- ymd_hms("2018-01-01 00:00:00")
        end <- ymd_hms("2018-06-15 00:00:00")
        
        if(sitename == "Desoto"){
          siteNo <- "06892350"
          pCodes <- c("99133", "00060", "00065", "00300", "00301","00010", 
                      "00095", "00400", "32295")
          data <- readNWISuv(siteNumbers = siteNo, parameterCd = pCodes, 
                           startDate = start, endDate = end, tz = "America/Chicago")
        
          # Rename columns from codes to readable names
          data <- renameNWISColumns(data)
          # Fix remaining names that are still parameter codes
          names(data)[names(data)=="00301_Inst"] <- "DO_percentsat" 
          names(data)[names(data)=="99133_Inst"] <- "nitrateNitrite"
          
          # Convert from American to SI units
          # cubic foot per second to m3 per second
          ft3s_m3s <- function(ft3s){
            m3s <- ft3s*0.0283168
            return(m3s)
          }
          # foot to meters
          ft_m <- function(ft){
            m <- 0.3048*ft
            return(m)
          }
          data$Flow_Inst <- ft3s_m3s(data$Flow_Inst)
          data$GH_Inst <- ft_m(data$GH_Inst)
          
          # drop agency code, siteNo, various quality codes
          dataframe <- data[c(-1,-2,-5,-7,-9,-11,-13,-15,-17,-19,-21,-22)]
          
          # add column for UTC time
          dataframe$dateTime_UTC <- dataframe$dateTime + hours(5)
        }
        # save the USGS data locally
        write.csv(dataframe, row.names = FALSE,
                  file = paste("KAWN_SensorData_", sitename, ".csv", sep = ""))
      }
  }
  return(dataframe)
  }

bowersock <- masterFile(sitename = "Bowersock", USGS = FALSE)
eric <- masterFile(sitename = "Eric", USGS = FALSE)
steve <- masterFile(sitename = "Steve", USGS = FALSE)
desoto <- masterFile(sitename = "Desoto", USGS = TRUE)
```

```{r reachQ}
#if(Q.avg = TRUE && Qdata.avg file exists){
  #loadfile
#}
#if(Q.avg = FALSE && Qdata file exists){
  #load file
#}
#if(!Qdata.avg file exists | !Qdata file exists){
  #if (Q.avg = TRUE){
    #create Q.avg file
    #save Q.avg
  #}
  #if(Q.avg = FALSE){
    #create Q file
    #save Q file
  #}
#}

# avg.Q: logical, TRUE returns dataframe of mean daily Q at all sites, FALSE returns all measurements of Q at all sites
# load from file: logical, if TRUE loads appropriate dataframe from file
Q.dataframe <- function(avg.Q){
  if(avg.Q == "TRUE" && file.exists("KAWN_USGSDischarge_dailymeans.csv")){
    Qdata <- read.csv("KAWN_USGSDischarge_dailymeans.csv", header = TRUE)
  }
  if(avg.Q == "FALSE" && file.exists("KAWN_USGSDischarge_instantaneous.csv")){
    Qdata <- read.csv("KAWN_USGSDischarge_instantaneous.csv", header = TRUE)
  }
  if(!file.exists("KAWN_USGSDischarge_dailymeans.csv") | !file.exists(file.exists("KAWN_USGSDischarge_dailymeans.csv"))){
    Q1.lawrence <- "06891080" # USGS identifier codes for gauges of interest
    Q3.wakarusa <- "06891500"
    Q4.stranger <- "06892000"
    Q5.desoto <- "06892350"
    sites <- c(Q1.lawrence, Q3.wakarusa, Q4.stranger, Q5.desoto)
    
    start <- ymd_hms("2018-01-01 00:00:00") # start date of data
    end <- ymd_hms("2018-06-15 00:00:00") # end date of data
    
    #pull from USGS server
    Qdata <- readNWISuv(siteNumbers = sites, parameterCd = "00060", # discharge 
                          startDate = start, endDate = end, tz = "America/Chicago")
    Qdata <- renameNWISColumns(Qdata)
    
    ft3s_m3s <- function(ft3s){
      m3s <- ft3s*0.0283168
    return(m3s)
      }
    Qdata$Flow_Inst <- ft3s_m3s(Qdata$Flow_Inst) # convert to SI units
    
    Qdata$dateTime <- ymd_hms(Qdata$dateTime) # report date in lubridate format
    Qdata <- Qdata[c(-1,-5,-6)] # take out the columns that report data quality codes
    
    if(avg.Q == "TRUE"){
      Qdata <- # if daily averages are requested, take the flow data and compute an average value for each day
        Qdata %>% 
        group_by(site_no, dateTime = floor_date(dateTime, "day")) %>%
          summarize(mean.Q = mean(Flow_Inst))
      
      Qdata <- ymd(Qdata$dateTime) # report date in lubridate format
    }
    
    Qdata <- spread(Qdata, site_no, mean.Q) #reorganize the dataframe so each site is in its own column
    
    names(Qdata)[names(Qdata)=="06891080"] <- "Q1.lawrence" 
    names(Qdata)[names(Qdata)=="06891500"] <- "Q3.wakarusa" 
    names(Qdata)[names(Qdata)=="06892000"] <- "Q4.stranger" 
    names(Qdata)[names(Qdata)=="06892350"] <- "Q5.desoto" 
    
    # read in flow data from farmland report
    FarmlandReport <- read.csv("FarmlandDailyReport_MCKformatted.csv", header = TRUE, stringsAsFactors = FALSE)
    Q2.Farmland <- FarmlandReport[c(-2:-8, -10:-11)]
    Q2.Farmland$Date <- mdy(Q2.Farmland$Date)
    Q2.Farmland <- Q2.Farmland[Q2.Farmland$Date >= ymd("2018-01-01"),] # only look at Jan 1 onwards
    
    # convert from gpm to m3s
    gpm_m3s <- function(gpm){
                m3s <- gpm*0.000063090196
                return(m3s)
    }
    Q2.Farmland$flowRate_Outfall001A_gpm <- gpm_m3s(Q2.Farmland$flowRate_Outfall001A_gpm)
    
    # merge Q2 with rest of Qdata.avg
    Qdata <- merge(Qdata, Q2.Farmland, by.x = "dateTime", by.y = "Date", all = TRUE)
    names(Qdata)[names(Qdata)=="flowRate_Outfall001A_gpm"] <- "Q2.Farmland"
    
    # pumping stops on apr 1
    Qdata$Q2.Farmland[Qdata$day > ymd("2018-04-01")] <- NULL
    
    #change na values to null
    Qdata[is.na(Qdata)] <- NULL
    
    # balance equation: Q5.desoto = Q1.lawrence + Q2.farmland + Q3.wakarusa + Q4.stranger
    Qdata$upstreamSum <- Qdata$Q1.lawrence + Qdata$Q2.Farmland + Qdata$Q3.wakarusa + Qdata$Q4.stranger
    # calculate amount off that the sum of upstream values is from downstream value
    Qdata$waterBal_percentDev <- ((Qdata$Q5.desoto - Qdata$upstreamSum) / Qdata$upstreamSum)*100
    
    if(Q.avg == "TRUE"){
      write.csv(Qdata, "KAWN_USGSDischarge_dailymeans.csv", row.names = FALSE)
    } else{
      write.csv(Qdata, "KAWN_USGSDischarge_instantaneous.csv", row.names = FALSE)
    }
  }
  return(Qdata)
  }
    
Qdata.avg <- Q.dataframe(Q.avg = TRUE)
Qdata.instant <- Q.dataframe(Q.avg = FALSE)






plot(y = Qdata.avg$Q5.desoto, x = Qdata.avg$upstreamSum, main = "Sum upstream Q vs downstream Q")
lm.Q <- summary(lm(Q5.desoto ~ upstreamSum, data = Qdata.avg))

plot(x = Qdata.avg$day, y = Qdata.avg$waterBal_percentDev, main = "Percent error of water balance")
```
percent deviation = ((downstream Q - upstream Q) / upstream Q) * 100%  
where downstream Q = desoto gage measurement  
      upstream Q = sum (sum Lawrence gauge, Farmland flow, Wakarusa R. gage, Stranger Creek gage)  

negative values: upstream has greater Q  

positive values: downstream (desoto gauge) has greater Q  
- if precipitation was entering reach, this will happen  
- gages on wakarusa and stranger are pretty far upstream, unaccounting for any gains below the gage (therefore, water balance will tend to predict positive percent deviations)  

mean = `r mean(na.omit(Qdata.avg$waterBal_percentDev))` percent of Q unaccounted for in water balance  

linear model of sum upstream Q vs downstream Q: R2 = `r lm.Q$r.squared` 


```{r dischargeEstimate}
Qdata <- spread(Qdata, site_no, Flow_Inst)
names(Qdata)[names(Qdata)=="06891080"] <- "Q1.lawrence" 
names(Qdata)[names(Qdata)=="06891500"] <- "Q3.wakarusa" 
names(Qdata)[names(Qdata)=="06892000"] <- "Q4.stranger" 
names(Qdata)[names(Qdata)=="06892350"] <- "Q5.desoto" 

# approximate discharge at site 2
# Q.site2 <- Q1.lawrence + Q2.farmland
Qdata$Q1.lawrence + Qdata$
# Q.site2 <- Qdata[c(1,)]

# approximate discharge at site 3
# Q.site3 <- Q1.lawrence + Q2.farmland

# NEXT STEPS: write function to ask if you want daily averaged data or not. save local file
```


```{r NEONpullCleanup, include=FALSE, eval=FALSE}
downloadNEONfiles <- FALSE

if (downloadNEONfiles == TRUE){
  dir.create(paste0(getwd(), "/NEONfiles"))
  
  ######## PAR ############
  getPackage(dpID = "DP1.00024.001", site_code = "UKFS",
           year_month = "2018-02", package = "basic",
           savepath = paste(getwd(), "NEONfiles", sep = "/")) # Feb at KU field station
  getPackage(dpID = "DP1.00024.001", site_code = "UKFS",
           year_month = "2018-03", package = "basic",
           savepath = paste(getwd(), "NEONfiles", sep = "/")) # March at KU field station
  #getPackage(dpID = "DP1.00024.001", site_code = "UKFS",
           #year_month = "2018-04", package = "basic",
           #savepath = getwd()) # April at KU field station 
  stackByTable(dpID = "DP1.00024.001", 
               filepath = paste0(getwd(), "/NEONfiles"), 
               savepath = paste0(getwd(), "/NEONfiles"), 
               folder = TRUE)
  
  ######## Barometric pressure ############
  getPackage(dpID = "DP1.00004.001", site_code = "UKFS",
           year_month = "2018-02", package = "basic",
           savepath = paste(getwd(), "NEONfiles", sep = "/")) # Feb at KU field station
  getPackage(dpID = "DP1.00004.001", site_code = "UKFS",
           year_month = "2018-03", package = "basic",
           savepath = paste(getwd(), "NEONfiles", sep = "/")) # March
  #getPackage(dpID = "DP1.00004.001", site_code = "UKFS",
           #year_month = "2018-04", package = "basic",
           #savepath = paste(getwd(), "NEONfiles", sep = "/")) # April
  stackByTable(dpID = "DP1.00004.001", 
               filepath = paste0(getwd(), "/NEONfiles"), 
               savepath = paste0(getwd(), "/NEONfiles"), 
               folder = TRUE)
}

PAR <- read.csv(file = paste0(getwd(), "/NEONfiles", "/stackedFiles", 
                              "/PARPAR_1min.csv"), header = TRUE)
PAR <- subset(PAR, subset = PAR$verticalPosition == 10) # take measurements that are closest to ground
PAR$startDateTime <- ymd_hms(PAR$startDateTime, tz = "America/Chicago")
PAR <- data.frame(
    dateTime = PAR$startDateTime,
    mean = PAR$PARMean,
    min = PAR$PARMinimum,
    max = PAR$PARMaximum,
    var = PAR$PARVariance
)

# PAR units are micromoles per m^2 per second

#### Barometric pressure, units are kPa
BarPress <- read.csv(file = paste0(getwd(), "/NEONfiles", "/stackedFiles", 
                              "/BP_30min.csv"), header = TRUE)
BarPress$startDateTime <- ymd_hms(BarPress$startDateTime, tz = "America/Chicago")
BarPress <- data.frame(
  dateTime = BarPress$startDateTime,
  mean = BarPress$staPresMean,
  min = BarPress$staPresMinimum,
  max = BarPress$staPresMaximum,
  var = BarPress$staPresVariance
)
# convert to mmHg
kpa_mmhg <- function(kpa){
  mmhg <- kpa*7.5006156130264
  return(mmhg)
}
BarPress$mean <- kpa_mmhg(BarPress$mean)
BarPress$min <- kpa_mmhg(BarPress$min)
BarPress$max <- kpa_mmhg(BarPress$max)
BarPress$var <- kpa_mmhg(BarPress$var)
```
