---
title: "Metabolism Modeling"
author: "Michelle Catherine Kelly"
output:
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: console
---

Copyright (c) 2019 Michelle Catherine Kelly  

License: MIT License  

```{r setup}
# grab streamPULSE pipeline tools
#install.packages("devtools")
#devtools::install_github('streampulse/StreamPULSE', dependencies=TRUE)

# grab latest version of streamMetabolizer
#remotes::install_github('appling/unitted')
#remotes::install_github("USGS-R/streamMetabolizer")

# load packages
library(StreamPULSE)
library(streamMetabolizer)
library(tidyverse)
library(lubridate)
```

```{r metabolism.load}
# Load in cleaned metabolism data from file
# Skip this block if you haven't run the metabolism model, or if you want 
# to re-compute the model
modelfit.S1S2 <- readRDS("./Outputs/MetabResults_TwoStation.RData")
modelfit.desoto <- readRDS("./Outputs/MetabResults_Desoto.RData")
```

```{r singleStation}
# Model stream metabolism from raw data using StreamPULSE database and 
# streamMetabolizer package

# Define the desired model type for streamMetabolizer
model_type <-  "bayes"
# Define the desired modeling framework 
model_name <-  "streamMetabolizer"

# Desoto ------------------------------------------------------
site_code.desoto <- "KS_KANSASR"
desoto_data <- request_data(site_code.desoto)
fitdata.desoto <- prep_metabolism(desoto_data, type = model_type, 
                                  model = model_name, rm_flagged = "Bad Data", 
                                  fillgaps = "interpolation",
                                  estimate_areal_depth = TRUE)
modelfit.desoto <- fit_metabolism(fitdata.desoto)



# Inspect model output
# We have 10 days where ER estimate is positive
# Positive ER estimates range from near 0 to ~5.5, some of these days also
# correspond to negative/near 0 GPP estimates
# Conclude that modeling is wonky for these days, exclude them from analysis

# 2018-02-25
modelfit.desoto$predictions[modelfit.desoto$predictions$date == "2018-02-25",][2:7] <- NA
# 2018-02-26
modelfit.desoto$predictions[modelfit.desoto$predictions$date == "2018-02-26",][2:7] <- NA
# 2018-02-27
modelfit.desoto$predictions[modelfit.desoto$predictions$date == "2018-02-27",][2:7] <- NA
# 2018-02-28
modelfit.desoto$predictions[modelfit.desoto$predictions$date == "2018-02-28",][2:7] <- NA
# 2018-03-01
modelfit.desoto$predictions[modelfit.desoto$predictions$date == "2018-03-01",][2:7] <- NA
# 2018-03-02
modelfit.desoto$predictions[modelfit.desoto$predictions$date == "2018-03-02",][2:7] <- NA
# 2018-03-03
modelfit.desoto$predictions[modelfit.desoto$predictions$date == "2018-03-03",][2:7] <- NA
# 2018-03-10
modelfit.desoto$predictions[modelfit.desoto$predictions$date == "2018-03-10",][2:7] <- NA
# 2018-03-11
modelfit.desoto$predictions[modelfit.desoto$predictions$date == "2018-03-11",][2:7] <- NA

saveRDS(modelfit.desoto, file = "./Outputs/MetabResults_Desoto.RData") 
write.csv(modelfit.desoto$predictions, file = "./Outputs/MetabResults_Desoto.csv")
```

```{r areal.extent}
# Compute gas exchange distance at S3
# From Stream Ecosystems in a Changing Environment: 
#   "One-station techniques measure a reach of stream that scales with the 
#   transport distance of oxygen; a proposed distance is about three times 
#   the transport distance of O2 (ie. 3V/K, Chapra and Di Toro 1991), which 
#   corresponds to 95% O2 turnover in the reach. Spatial variability at the 
#   spatial scale of this distance will bias estimates of one-station 
#   metabolism. For example, a sonde placed in a tailwater below a dam may 
#   measure oxygen processes both in the tailwater and in the upstream lake."

# L = 3 v / K600
# Where L = distance along river at which 95% of oxygen has turned over [m]
#       v = average flow velocity [m / day]
#       K600 = gas exchange coefficient predicted by model [day^-1]
# Citations: Chapra & Del Toro 1991
#            Hall et al. 2012
#            Jones & Stanley Stream Ecosystems in a Changing... pg 158

# Grab K600 values from the metabolism modeling data -------------------------
# StreamMetabolizer uses the 50% condfidence interval (not the mean) GPP and 
# ER measurements, which is why I'm grabbing the 50% confidence interval K600
# measurements
K600.desoto <- 
  modelfit.desoto[["fit"]]@fit[["daily"]] %>%
  select(date, K600_daily_50pct) %>%
  filter(date < ymd("2018-05-01") & date >= ymd("2018-02-01")) 

# Interpolate for velocity using v = Q/A -------------------------------------
# Approximate river cross sectional A using interpolated depth and aerial w
# Grab river width - this is from google maps data, same values used in 
# uptake analysis
w.Eric <- 210.9
w.Steve <- 113.4
w.Desoto <- 138.07
# Calculate water velocity based on modeled depth and discharge
v.S1 <- 
  S1_preproc %>%
  select(datetime, depth, discharge) %>%
  group_by(date = date(datetime)) %>%
  filter(date < ymd("2018-05-01") & date >= ymd("2018-02-01")) %>%
  summarise(depth_m = mean(depth), Q_m3s = mean(discharge), 
            A_m2 = w.Eric*depth_m, v_ms = Q_m3s/A_m2,
            v_mday = v_ms*86400)
v.S2 <- 
  S2_preproc %>%
  select(datetime, depth, discharge) %>%
  group_by(date = date(datetime)) %>%
  filter(date < ymd("2018-05-01") & date >= ymd("2018-02-01")) %>%
  summarise(depth_m = mean(depth), Q_m3s = mean(discharge), 
            A_m2 = w.Steve*depth_m, v_ms = Q_m3s/A_m2,
            v_mday = v_ms*86400)
v.desoto <- 
  modelfit.desoto[["fit"]]@data %>%
  select(date, solar.time, depth, discharge) %>%
  group_by(date) %>%
  filter(date < ymd("2018-05-01") & date >= ymd("2018-02-01")) %>%
  summarise(depth_m = mean(depth), Q_m3s = mean(discharge), 
            A_m2 = w.Desoto*depth_m, v_ms = Q_m3s/A_m2,
            v_mday = v_ms*86400)
# Run oxygen turnover distance calculation
K600.desoto <- 
  full_join(K600.desoto, v.desoto) %>%
  mutate(O2TurnL_m = 3*v_mday/K600_daily_50pct, O2TurnL_km = O2TurnL_m/1000)

# Summary stats --------------------------------------------------------------
v.S1 %>% summarise_all(mean, na.rm = T)
v.S2 %>% summarise_all(mean, na.rm = T)
K600.desoto %>% summarise_all(mean, na.rm = T)

# Save as CSV files ----------------------------------------------------------
saveRDS(v.S1, file = "./Outputs/Velocity_S1.RData")
write.csv(v.S1, file = "./Outputs/Velocity_S1.CSV", row.names = F)

saveRDS(v.S2, file = "./Outputs/Velocity_S2.RData")
write.csv(v.S2, file = "./Outputs/Velocity_S2.CSV", 
          row.names = F)

saveRDS(K600.desoto, file = "./Outputs/OxygenTurnover_Desoto.RData")
write.csv(K600.desoto, file = "./Outputs/OxygenTurnover_Desoto.CSV", 
          row.names = F)
```

```{r twoStation}
# Significant portions of the below script are adapted/sourced from Hall et al 
# 2016 supplemental material

########### Load libraries ####################################################
library(mcmc)
library(chron)
set.seed(1) # for reproducability

########### Internal Functions for Two Station Modeling #######################
# Function to estimate K at temperature 'temp'from K600.  Wanninkhof 1992.
Kcor <- function (temp, K600) {
	K600 / (600/(1800.6-(temp*120.1)+(3.7818*temp^2)-(0.047608*temp^3)))^-0.5
}

# tspost FUNCTION: Calculate posterior probablility of model
# This function calculates the posterior probability of the model given 
# parameters
# Called within twostationpostsum
#
# ARGUMENTS:
# MET          Dataframe name of cleaned raw two station data (ex. "TS_S1S2")
# tempup        Temperature data from upstream station [deg C]
# tempdown      Temperature data from downstream station [deg C]
# oxyup         Oxygen data from upstream station [mg-O2/L]
# oxydown       Oxygen data from downstream station [mg-O2/L]
# Light         [any light unit]
# tt            travel time [days]
# Kmean   
# Ksd
tspost <- function(MET, tempup, tempdown, oxyup, oxydown, light, tt, z, osat, 
                   Kmean, Ksd, ...){
	# Assign the paramters we solve for to easy to understand values
	GPP <- MET[1]
	ER <- MET[2]
	K <- MET[3]
	# Always model the log of variance so that one does not get a 
	# negative standard deviation
	sigma <- exp(MET[4]) 
	
	lag <- as.numeric(round(tt/0.0104166667))
	
	metab <- vector(mode = "numeric", length = length(oxyup)) #create empty vector

  # Below is equation 4 in the paper, solving for downstream O2 at each
	# interval. It references other functions:  Kcor converts K600 to KO2 
	# for a given temperature. 
  for (i in 1:length(oxyup)){
    metab[i] <- (oxyup[i] + ((GPP/z)*(sum(light[i:(i+lag)])/sum(light))) + 
                   ER*tt/z + 
                   (Kcor(tempup[i],Kmean))*tt*(osat[i] - 
                                             oxyup[i] + 
                                             osat[i])/2) / 
      (1+ Kcor(tempup[i],Kmean)*tt/2) 
    }
	
	# likelhood is below.  dnorm caculates the probablity density of a normal 
	# distribution, note log.
	loglik <- sum(dnorm(oxydown, metab, sigma, log=TRUE))
	
	# Priors, note wide distributions for GPP and ER
	prior <- (dnorm(GPP, mean=10, sd=10, log=TRUE)) + 
	  (dnorm(ER, mean=-10, sd=10, log=TRUE)) + 
	  (dnorm(K, mean=Kmean, sd=Ksd, log=TRUE))
	
	return(loglik + prior)
}

# O2TimeSeries FUNCTION: Return modeled oxygen time series from median GPP, 
# ER estimates
# ARGUMENTS:
# GPP
# ER
# O2data    Dataframe of cleaned raw two station data (ex. "TS_S1S2")
# Kmean
# z
# tt
# upName        Character name of upstream station (ex. "S1")
# downName      Character name of downstream station (ex. "S2")
O2TimeSeries <- function(GPP, ER, O2data, Kmean, z, tt, upName, downName) {
  # Ungroup O2data
  O2data <- O2data %>% ungroup()
  
	#number of 5 min readings bewteen up and down probe corresponding 
  # to travel time tt
	lag <- as.numeric(round(tt/0.0104166667))
	
	# trim the ends of the oxy and temp data by the lag so that oxydown[1] 
	# is the value that is the travel time later than oxy up. The below 
	# calls are designed to work with our data structure.
	
	# Seperate data into upstream and downstream sections
  updata <- O2data[O2data$`river station` == upName,]
  downdata <- O2data[O2data$`river station` == downName,]
	
	tempup <- updata$temp[1:as.numeric(length(updata$temp)-lag)] # trim the end by the lag
	tempdown <- downdata$temp[(1+lag):length(downdata$temp)]
	
	oxyup <- updata$oxy[1:as.numeric(length(updata$temp)-lag)]
	# define osat 
	osat <- updata$DO.sat[1:as.numeric(length(updata$temp)-lag)]
	oxydown <- downdata$oxy[(1+lag):length(downdata$temp)]
	
	timeup <- updata$dtime[1:(length(updata$temp)-lag)]
	timedown <- downdata$dtime[(1+lag):length(downdata$temp)]
	
	light <- downdata$light
  
	# Initialize an empty vector
	modeledO2 <- numeric(length(oxyup))
	# Calculate metabolism at each timestep
	for (i in 1:length(oxyup)) {
	  modeledO2[i] <- (oxyup[i] + ((GPP/z)*(sum(light[i:(i+lag)]) / sum(light))) +
	                 ER*tt/z + 
	                 (Kcor(tempup[i],Kmean))*tt*(osat[i] - 
	                                               oxyup[i] + 
	                                           osat[i])/2) / 
	    (1 + Kcor(tempup[i],Kmean)*tt/2) 
	}
	
	# Convert time from chron to posixct
	timeup <- as.POSIXlt(timeup)
	timedown <- as.POSIXlt(timedown)
	
  oxymodel <- data.frame(timeup, timedown, oxydown, oxyup, modeledO2)
  return(oxymodel)
}

# Two station modeling function: Calculate MCMC for data 
# This function prepares the data for any given river, runs the MCMC and 
# returns posterior distributions
# ARGUMENTS:
# data          Dataframe name of cleaned raw two station data (ex. "TS_S1S2")
# upName        Character name of upstream station (ex. "S1")
# downName      Character name of downstream station (ex. "S2")
# start         First guess at GPP, ER ranges
# z             average river depth [m]
# tt            travel time [days]
# Kmean         mean K for study period
# Ksd           standard deviation of K
# nbatch        number of MCMC trials
# scale         
twostationpostsum <- function(O2data, upName, downName, start, z, tt, Kmean, Ksd, 
                              nbatch, scale) {
  # Add date column
  O2data <- O2data %>% group_by(date = date(datetime))
  # Create list of unique dates in data
  dateList <- unique(O2data$date)
  
  # Initialize empty vectors to store results from modeling
  #date <- vector(length = length(unique(data$date)))
  GPP <- vector(mode = "numeric", length = length(unique(O2data$date)))
  GPP.lower <- vector(mode = "numeric", length = length(unique(O2data$date)))
  GPP.upper <- vector(mode = "numeric", length = length(unique(O2data$date)))
  ER <- vector(mode = "numeric", length = length(unique(O2data$date)))
  ER.lower <- vector(mode = "numeric", length = length(unique(O2data$date)))
  ER.upper <- vector(mode = "numeric", length = length(unique(O2data$date)))
  K <- vector(mode = "numeric", length = length(unique(O2data$date)))
  K.lower <- vector(mode = "numeric", length = length(unique(O2data$date)))
  K.upper <- vector(mode = "numeric", length = length(unique(O2data$date)))
  s <- vector(mode = "numeric", length = length(unique(O2data$date)))
  s.lower <- vector(mode = "numeric", length = length(unique(O2data$date)))
  s.upper <- vector(mode = "numeric", length = length(unique(O2data$date)))
  accept <- vector(length = length(unique(O2data$date)))
  
  # For loop iterating through each day of data
  for (i in 1:length(dateList)){
    # Subset data to single day of interest (ith date in date string)
    data <- subset(O2data, date == dateList[i])
    # Seperate data into upstream and downstream sections
    updata <- data[data$`river station` == upName,]
    downdata <- data[data$`river station` == downName,]
    
    # NOTE: In Hall et al, the number below was 0.00347222, which corresponds to:
    # 0.00347222 days = 5 minutes, as their O2 sensors took 5-minute readings.
    # Here, our sensors took 15 minute readings. So:
    # 15 minutes = 0.0104166667 days
    # number of 15 min readings bewteen up and down probe corresponding to travel 
    # time tt
    lag <- as.numeric(round(tt/0.0104166667))
    
    # trim the ends of the oxy and temp data by the lag so that oxydown[1] 
    # is the value that is the travel time later than oxy up.  The below calls 
    # are designed to work with our data structure.
    
    if (length(updata$temp) < lag) {
      # If there is less data during a day than the lag interval, move to next day
      message("ERROR: model not computed for ", dateList[i], " as insufficient observations provided.")
      break
    } else{
      # Else continue
      tempup <- updata$temp[1:as.numeric(length(updata$temp)-lag)] # trim the end by the lag
    	oxyup <- updata$oxy[1:as.numeric(length(updata$temp)-lag)]
    	# define osat 
    	osat <- updata$DO.sat[1:as.numeric(length(updata$temp)-lag)]
    	
    	tempdown <- downdata$temp[(1+lag):length(downdata$temp)]
    	oxydown <- downdata$oxy[(1+lag):length(downdata$temp)]
    	
    	light <- downdata$light
    	
    	# perform MCMC
    	# see documentation on mcmc
    	met.post <- metrop(tspost, initial = start, nbatch = nbatch, scale = scale, 
    	                   tempup = tempup, tempdown = tempdown, oxyup = oxyup, 
    	                   osat = osat,
    	                   oxydown = oxydown,  z = z, light = light, tt = tt, 
    	                   Kmean = Kmean, Ksd = Ksd, debug = TRUE)
    	
    	# trying to troubleshoot here
      print(plot(ts(met.post$batch), main = dateList[i]))
    	
    	# Calculate overall estimates for each day
      gppr <- quantile(met.post$batch[(2000:nbatch),1], c(0.025, 0.5, 0.975))
      err <- quantile(met.post$batch[(2000:nbatch),2], c(0.025, 0.5, 0.975))
      Kr <- quantile(met.post$batch[(2000:nbatch),3], c(0.025, 0.5, 0.975))
      sr <- quantile(met.post$batch[(2000:nbatch),4], c(0.025, 0.5, 0.975))
      
      # Add results to vectors
      #date[i] <- ymd(unique(data$date)[i])
      GPP[i] <- gppr[2]
      GPP.lower[i] <- gppr[1]
      GPP.upper[i] <- gppr[3]
      ER[i] <- err[2]
      ER.lower[i] <- err[1]
      ER.upper[i] <- err[3]
      K[i] <- Kr[2]
      K.lower[i] <- Kr[1]
      K.upper[i] <- Kr[3]
      s[i] <- sr[2]
      s.lower[i] <- sr[1]
      s.upper[i] <- sr[3]
      accept[i] <- met.post$accept # log likelihood plus priors, should be about 0.2
    }
    i <- i+1
  }
  # Create dataframe of predicted metabolism values
  pred.metab <- data.frame(date = dateList, GPP, GPP.lower, GPP.upper, ER, ER.lower, 
                            ER.upper, K, K.lower, K.upper, s, s.lower, s.upper)
  
  # Call O2TimeSeries function to return modeled O2 values based on median GPP and ER
  # modeling results
  oxymodel <- O2TimeSeries(GPP = pred.metab$GPP, ER = pred.metab$ER, 
                           O2data = O2data, Kmean = Kmean, z = z, tt = tt, 
                           upName = upName, downName = downName)
  
  # Create output list to return to user
  output <- list(pred.metab = pred.metab, accept = accept, oxymodel = oxymodel)
  
  return(output)
}

# nightreg function for nighttime regression.  Reports K600, units of 1/d.
# Make sure you set the times approprately, i.e. just after dark to whenever 
# deltaO2 reaches 0

# Convert KO2 to K600
K600fromO2 <- function(temp, KO2) {
  ((600/(1800.6-(120.1*temp)+(3.7818*temp^2)-(0.047608*temp^3)))^-0.5)*KO2
}

# nightStart    Time (in HH:MM:SS) that nighttime begins in solar time
# nightEnd      Time (in HH:MM:SS) that nighttime ends in solar time
nightreg <- function(o2file, nightStart, nightEnd, downName) {
  # Section off the data so we're only dealing with the downstream data
  o2file <- o2file[o2file$`river station` == downName,]
  
  ## Trim the daily data so it's only during nighttime
  # Create decimal time column
  o2file$decimalTime <- hour(o2file$datetime) + minute(o2file$datetime)/60
  # Calculate hours since noon, where 0 would = noon, 1 = 1PM, 2 = 2PM... 12 = midnight
  o2file$timeSinceNoon <- o2file$decimalTime - 12
  # Convert nightStart to decimal time since noon
  nightStart <- hms(nightStart)
  nightStart <- (hour(nightStart) + minute(nightStart)/60) - 12
  # Convert nightEnd to decimal time since noon
  nightEnd <- hms(nightEnd)
  nightEnd <- (hour(nightEnd) + minute(nightEnd)/60) - 12
  # Subset nighttime data
  o2night <- o2file[data.table::between(o2file$timeSinceNoon, nightStart, 12) | 
                      data.table::between(o2file$timeSinceNoon, -12, nightEnd),]
  
  # Add column to distinguish night time `days` of data
  # We'll then loop through these `days` to calculate K estimate
  # on each `day`
  o2night <- 
    o2night %>%
    mutate(nightBlock = case_when(timeSinceNoon < 0 ~ date(datetime) - 1,
                                  timeSinceNoon > 0 ~ date(datetime)))
  # Create list of unique nightBlocks
  nightList <- unique(o2night$nightBlock)
  
  # Initialize vectors to store results
  regIntercept <- vector(mode = "numeric", length = length(nightList))
  regSlope <- vector(mode = "numeric", length = length(nightList))
  K600 <- vector(mode = "numeric", length = length(nightList))
  r2fit <- vector(mode = "numeric", length = length(nightList))
  #satdeficit <- vector(mode = "numeric", length = nrow(o2night))
  #deltaoxy <- vector(mode = "numeric", length = nrow(o2night))
  
  # Need some way to group by night blocks
  for (i in 1:length(nightList)) {
    # Subset data to single nightblock of interest
    data <- subset(o2night, nightBlock == nightList[i])
    
    # Create lists of temp, oxy, oxysat for each nighttime block
    temp <- data$temp
    oxy <- data$oxy
    oxysat <- data$DO.sat
    
    # Calculate moving average on oxy data
    oxyf1 <- stats::filter(as.numeric(data$oxy), rep(1/3, 3), sides=2)
    # trim the ends of the oxy data
    oxyf2 <- oxyf1[c(-1,-length(oxyf1))]
    # calculate delO/delt
    deltaO2 <- ((oxyf2[-1] - oxyf2[-length(oxyf2)])/15)*1440 ## CHECK: IS /5 B/C THEY HAVE 5 MIN SAMPLING TIME? this is my hunch, so I've adjusted to /15
    
    # Trim the first two and last one from the oxysat data to match the filter oxy data
    oxysattrim <- oxysat[c(-2:-1, -length(oxyf2))]
    # Calc the dodef
    satdef <- oxysattrim - oxyf2[-1]
    
    # Calculate regression
    # # currently set to exclude NA data from the regression
    nreg <- lm(deltaO2 ~ satdef, na.action = na.exclude)
    coeff <- coef(nreg)
    # Get the R2 of the regression
    r2 <- summary(nreg)$r.squared
    # Convert the slope of the regression to K600 value
    calcK600 <- as.numeric(K600fromO2(mean(temp), coeff[2]))
    
    # Save results to lists
    regIntercept[i] <- as.numeric(coeff[1])
    regSlope[i] <- as.numeric(coeff[2])
    K600[i] <- calcK600
    r2fit[i] <- r2
    plot(satdef,deltaO2, main = nightList[i])
    abline(nreg)
  }
  # Create dataframe of daily estimates
  dailyEstimates <- data.frame(date = nightList, intercept = regIntercept, 
                               slope = regSlope, K600 = K600, r2fit = r2fit)
  
  # Return daily estimates and mean K600 for period
  out <- list(dailyEstimates = dailyEstimates, meanK600 = mean(dailyEstimates$K600))
  return(out)
}

########### 1. Load raw data from StreamPulse #################################
## Define time range
start <- "2018-02-01"
end <- "2018-05-01"

## S1 ("Eric")
site_code <- "KS_KANSASREASTLAWRENCE"
S1 <- request_data(site_code, startdate = start, enddate = end)
## S2 ("Steve")
site_code <- "KS_KANSASRFALLLEAF"
S2 <- request_data(site_code, startdate = start, enddate = end)

########### 2. Preprocess data using StreamPULSE functions ####################
# Format data for modeling
S1_preproc <- prep_metabolism(S1, rm_flagged = "Bad Data", 
                                fillgaps = "interpolation",
                                estimate_areal_depth = TRUE)
S2_preproc <- prep_metabolism(S2, rm_flagged = "Bad Data", 
                                fillgaps = "interpolation",
                                estimate_areal_depth = TRUE)

########## 3. Rename columns to match script format ###########################
# First actual DO log for S1 is at 2018-02-25 02:55:16
S1_preproc <- 
  S1_preproc$data %>%
  as_tibble() %>%
  rename(datetime = solar.time, temp = temp.water, oxy = DO.obs) %>%
  filter(datetime >= ymd_hms("2018-02-25 02:55:16", tz = "UTC") &
                             datetime < ymd_hms("2018-04-20 14:40:00", tz = "UTC"))
S2_preproc <- 
  S2_preproc$data %>%
  as_tibble() %>%
  rename(datetime = solar.time, temp = temp.water, oxy = DO.obs) %>%
  filter(datetime >= ymd_hms("2018-02-25 02:55:16", tz = "UTC") &
           datetime < ymd_hms("2018-04-20 14:40:00", tz = "UTC"))

# Remove seconds in time, this way datetimes will be equal
second(S1_preproc$datetime) <- 0
second(S2_preproc$datetime) <- 0
# Add column for site
S1_preproc$`river station` <- "S1"
S2_preproc$`river station` <- "S2"
# Merge dataframes
TS_S1S2 <- full_join(S1_preproc, S2_preproc)

# Function to convert time column to chron object
timeConvert <- function(data){
  # Mutate time column
  data <- 
    data %>%
    mutate(date = date(datetime), 
           time = paste(hour(datetime), minute(datetime), second(datetime), 
                        sep = ":"))
  # Convert to chron object
  dtime <- chron(dates = as.character(data$date), 
               times = as.character(data$time),
               format = c(dates = "y-m-d", times = "h:m:s"))
  return(dtime)
}
# Convert datetime to chron
TS_S1S2$dtime <- timeConvert(TS_S1S2)

# Load in calculated water velocity, area for travel time calculations
S1_v <- readRDS(file = "./Outputs/Velocity_S1.RData")
S2_v <- readRDS(file = "./Outputs/Velocity_S2.RData")

# Filter for dates of interest, join dataframes, at each date compute the mean 
# of velocity at S1 and S2
v_S1S2 <- 
  left_join(S1_v %>% filter(date >= date("2018-02-25")),
          S2_v %>% filter(date >= date("2018-02-25")), 
          by = "date", suffix = c(".S1", ".S2")) %>%
  select(date, v_ms.S1, v_ms.S2)
# Compute the mean v of S1 and S2 on each date
v_S1S2$v_ms.mean <- apply(v_S1S2[c("v_ms.S1","v_ms.S2")], MARGIN = 1, 
                          FUN = mean)

# Join depth data
z_S1S2 <- 
  left_join(S1_v %>% filter(date >= date("2018-02-25")),
          S2_v %>% filter(date >= date("2018-02-25")), 
          by = "date", suffix = c(".S1", ".S2")) %>%
  select(date, depth_m.S1, depth_m.S2)
# Compute the mean z of S1 and S2 on each date
z_S1S2$z.mean <- apply(z_S1S2[c("depth_m.S1","depth_m.S2")], 
                       MARGIN = 1, FUN = mean)

########### Inspect nighttime regression for K600 estimates ###################

# Section time series into the time chunks when both sites have data
TS_S1S2_a <- 
  TS_S1S2 %>%
  filter(datetime < ymd_hms("2018-03-05 05:40:00"))
TS_S1S2_b <- 
  TS_S1S2 %>%
  filter(datetime > ymd_hms("2018-03-12 16:40:00") & 
           datetime < ymd_hms("2018-03-28 00:55:00"))
TS_S1S2_c <- 
  TS_S1S2 %>%
  filter(datetime > ymd_hms("2018-03-28 23:55:00") &
           datetime < ymd_hms("2018-03-30 10:55:00"))
TS_S1S2_d <- 
  TS_S1S2 %>%
  filter(datetime > ymd_hms("2018-04-09 08:55:00") &
           datetime < ymd_hms("2018-04-14 08:25:00"))
TS_S1S2_e <- 
  TS_S1S2 %>%
  filter(datetime > ymd_hms("2018-04-17 18:10:00"))

k600_a <- nightreg(o2file = TS_S1S2_a, nightStart = "17:45:00", 
                     nightEnd = "06:30:00", downName = "S2")
k600_b <- nightreg(o2file = TS_S1S2_b, nightStart = "17:45:00", 
                     nightEnd = "06:30:00", downName = "S2")
k600_c <- nightreg(o2file = TS_S1S2_c, nightStart = "17:45:00", 
                     nightEnd = "06:30:00", downName = "S2")
k600_d <- nightreg(o2file = TS_S1S2_d, nightStart = "17:45:00", 
                     nightEnd = "06:30:00", downName = "S2")
k600_e <- nightreg(o2file = TS_S1S2_e, nightStart = "17:45:00", 
                     nightEnd = "06:30:00", downName = "S2")

k600 <-
  bind_rows(k600_a$dailyEstimates, k600_b$dailyEstimates, 
            k600_c$dailyEstimates, k600_d$dailyEstimates, 
            k600_e$dailyEstimates)

#write_csv(k600, path = "KawN_K600_NighttimeO2Regression.csv")

# After reviewing k600 models, decide to cut out models with fit (R2) of 
# 1 standard deviation below the mean (or lower)

# Display which estimates will be removed under this criteria
#k600[k600$r2fit < mean(k600$r2fit) - sd(k600$r2fit),]
# Return dates which are to be removed
#k600$date[k600$r2fit < mean(k600$r2fit) - sd(k600$r2fit)]

# Create new column denoting if the r2 value is kept or discarded,
# if discarded, NA the K600 value and fill with 
k600 <- 
  k600 %>%
  mutate(fitCriteria = ifelse(r2fit < mean(r2fit) - sd(r2fit), "DISCARD", 
                              "KEEP"),
         K600_adj = ifelse(fitCriteria == "DISCARD", NA, K600))
# Fill NA in adjusted K600 column with mean of remaining K600 values
k600$K600_adj[is.na(k600$K600_adj)] <- mean(k600$K600_adj, na.rm = TRUE)

unique(date(TS_S1S2_a$datetime))
sum(date(TS_S1S2$datetime) == ymd("2018-04-17", tz = "America/Chicago"))

########### Run Function ######################################################
### Create S1S2model function to reduce amt of temporary data saved to environment ##
S1S2model <- function(metabdata, vdata, zdata, kdata, nbatch){
  # Initiate empty lists
  dates <- list()
  traveltime_d <- list()
  depth_m <- list()
  k600_mean <- list()
  k600_sd <- list()
  velocity <- list()
  model_accept <- list()
  model_metab <- list()
  model_O2 <- list()
  
  for (i in seq_along(unique(date(metabdata$datetime)))){
    # Return date
    modelDate <- unique(date(metabdata$datetime))[i]
    
    # Return data for selected date
    metabd_select <- metabdata[date(metabdata$datetime) == modelDate,]
    vdata_select <- vdata[vdata$date == modelDate,]
    zdata_select <- zdata[zdata$date == modelDate,]
    kdata_select <- kdata[kdata$date == modelDate,]
    
    # If not a full day of data, skip
    # Readings every 15 min = 96 readings / day * 2 stations = 192 rows
    if(sum(date(metabd_select$datetime) == modelDate) < 192) {
      message("NOTE: Date ", modelDate," contains less than a full day of obervations and was skipped")
      next
    }
    
    ## 4. Estimate travel time during each date period ########################
    # Travel time = time for a parcel of H2O to travel from S1 to S2
  
    # Variables needed:
    # Mean H2O velocity at S1 (where v = Q/A, see equations in areal.extent chunk) 
    # Mean H2O velocity at S2
    # Distance between S1 and S2 = 5.13 km = 5130 m
    
    # Travel time [s] = distance [m] / velocity [m/s]
    # Convert from sec to hrs
    # Convert from sec to days
    vdata_select$tt_sec <- 5130 / vdata_select$v_ms.mean
    vdata_select$tt_hours <- vdata_select$tt_sec / 3600
    vdata_select$tt_days <- vdata_select$tt_sec / 86400
    # Save travel time as seperate variable
    tt_days <- vdata_select$tt_days
    
    ## 5. Get depth for date ##################################################
    z_m <- zdata_select$z.mean
    
    ## 6. Grab K600 from previously run nighttime regression ##################
    k <- kdata_select$K600_adj
    # Use SD of all K600 estimates
    k_sd <- sd(kdata$K600_adj)
    
    ## 7. Run 2-station metabolism modeling function ##########################
    metabout <- twostationpostsum(start = c(10, -10, 7, -2.2), 
                                  O2data = metabd_select, z = z_m, 
                                  tt = tt_days, Kmean = k, Ksd = k_sd, 
                                  upName = "S1", downName = "S2", 
                                  nbatch = nbatch, scale = 0.3)
    
    ## 8. Output data
    dates[[i]] <- modelDate
    traveltime_d[[i]] <- tt_days
    depth_m[[i]] <- z_m
    k600_mean[[i]] <- k
    k600_sd[[i]] <- k_sd
    
    velocity[[i]] <- vdata_select
    model_accept[[i]] <- metabout$accept
    model_metab[[i]] <- metabout$pred.metab
    model_O2[[i]] <- metabout$oxymodel
  }
  # Return model inputs
  modelinputs_k600 <- kdata
  modelinputs_v <- vdata
  # Combine metabolism results dataframe
  results <- 
    model_metab %>%
    bind_rows()
  results$accept <- unlist(model_accept)
  # Combine modeled O2 vs actual O2 dataframe
  modeledO2 <-
    model_O2 %>% 
    bind_rows()
  
  # Return to user
  return(list(results = results,
              modeledO2 = modeledO2,
              modelinputs_v = modelinputs_v,
              modelinputs_k600 = modelinputs_k600))
}

# Run function
metabmodel_a <- S1S2model(metabdata = TS_S1S2_a, vdata = v_S1S2, 
                          zdata = z_S1S2, kdata = k600, nbatch = 1e5)
metabmodel_b <- S1S2model(metabdata = TS_S1S2_b, vdata = v_S1S2, 
                          zdata = z_S1S2, kdata = k600, nbatch = 1e5)
metabmodel_c <- S1S2model(metabdata = TS_S1S2_c, vdata = v_S1S2, 
                          zdata = z_S1S2, kdata = k600, nbatch = 1e5)
metabmodel_d <- S1S2model(metabdata = TS_S1S2_d, vdata = v_S1S2, 
                          zdata = z_S1S2, kdata = k600, nbatch = 1e5)
metabmodel_e <- S1S2model(metabdata = TS_S1S2_e, vdata = v_S1S2, 
                          zdata = z_S1S2, kdata = k600, nbatch = 1e5)

# Check modeled oxygen
ggplot(data = metabmodel_a$modeledO2) +
  geom_line(aes(x = timeup, y = oxyup, color = "S1")) +
  geom_line(aes(x = timedown, y = oxydown, color = "S2")) +
  geom_line(aes(x = timedown, y = modeledO2, color = "Modeled O2"))

# Check modeled GPP and ER
ggplot(data = metabmodel_b$results) +
  geom_ribbon(aes(x = date, ymin = GPP.lower, ymax = GPP.upper, 
                  fill = "GPP"), alpha = 0.5) +
  geom_point(aes(x = date, y = GPP, color = "GPP")) +
  geom_point(aes(x = date, y = ER, color = "ER")) +
  geom_ribbon(aes(x = date, ymin = ER.lower, ymax = ER.upper, 
                  fill = "ER"), alpha = 0.5) +
  geom_hline(yintercept = 0, linetype = 2) +
  labs(y = "GPP or ER") +
  theme_classic()

metabmodel <- metabmodel_a$results %>%
  full_join(., metabmodel_b$results) %>%
  full_join(., metabmodel_c$results) %>%
  full_join(., metabmodel_d$results) %>%
  full_join(., metabmodel_e$results)

b <- metabmodel_a$modeledO2 %>%
  full_join(., metabmodel_b$modeledO2) %>%
  full_join(., metabmodel_c$modeledO2) %>%
  full_join(., metabmodel_d$modeledO2) %>%
  full_join(., metabmodel_e$modeledO2)

ggplot(data = b) +
  geom_line(aes(x = timeup, y = oxyup, color = "S1")) +
  geom_line(aes(x = timedown, y = oxydown, color = "S2")) +
  geom_line(aes(x = timedown, y = modeledO2, color = "Modeled O2")) +
  labs(x = "date", y = "O2") +
  theme_classic()

ggplot(data = metabmodel) +
  geom_ribbon(aes(x = date, ymin = GPP.lower, ymax = GPP.upper, 
                  fill = "GPP"), alpha = 0.5) +
  geom_point(aes(x = date, y = GPP, color = "GPP")) +
  geom_point(aes(x = date, y = ER, color = "ER")) +
  geom_ribbon(aes(x = date, ymin = ER.lower, ymax = ER.upper, 
                  fill = "ER"), alpha = 0.5) +
  geom_hline(yintercept = 0, linetype = 2) +
  labs(y = "GPP or ER") +
  theme_classic()


# Create dataframe of dates based on S3 modeled dates
#dateSeq <- tibble(date = seq(ymd('2018-01-31'), ymd('2018-05-31'), by='1 day'))
# Merge data
modelfit.S1S2 <-full_join(tsout_a$pred.metab, tsout_b$pred.metab)

# Remove dates containing partial data
# 2018-03-05
# 2018-03-12
# 2018-03-28
modelfit.S1S2[2:13][modelfit.S1S2$date == "2018-03-05",] <- NA
modelfit.S1S2[2:13][modelfit.S1S2$date == "2018-03-12",] <- NA
modelfit.S1S2[2:13][modelfit.S1S2$date == "2018-03-28",] <- NA

# Save joined data
saveRDS(metabmodel, file = "./Outputs/MetabResults_TwoStation.RData")
```

```{r NEP.calculate}
# Calculate Net Ecosystem Production (NEP) from metabolism model output

# Arguments
# model_output    dataframe returned by metabolismModeling
# Returns
# dataframe of date, cumulative NEP, cumulative GPP, and cumulative ER for site

NEP_calcs <- function(model_output){
  NEP_data <- data.frame(date = model_output$date,
                         GPP = model_output$GPP,
                         ER = model_output$ER)
  # to perform cumulative sum correctly, any NA values must be set to 0
  NEP_data[is.na(NEP_data)] <- 0
  # take cumulative sum
  NEP_data <- data.frame(date = NEP_data$date,
                         GPP_sum = cumsum(NEP_data$GPP),
                         ER_sum = cumsum(NEP_data$ER))
  # GPP + ER = NEP
  NEP_data$NEP <- rowSums(NEP_data[, c(2,3)])
  return(NEP_data)
}

# NEP calculations
NEP.S1S2 <- NEP_calcs(modelfit.S1S2)
NEP.desoto <- NEP_calcs(modelfit.desoto$predictions)
```

```{r stoich.scaling}
# Predict areal N uptake rate based on metabolism
# Assumptions: (Hall & Tank 2003)
# 1. C production will drive autotrophic & heterotrophic demand for N (AKA, 
#    assume a respiratory quotient = 1)
# 2. Molar C:N scaling ratio = 20 [g C / g N] (Hall & Tank 2003) OR 
#    Molar C:N scaling ratio = 12 [g C / g N] (Covino et al 2018, citing 
#    steltzer & lamberti 2001)
# 3. Net autotrophic production is 0.5*GPP (Odum 1957, Webster & Myer 1997)
# 4. Heterotrophic respiration can be estimated from ecosystem respiration
#    using the equation HR = ER - ra*GPP where HR is herotrophic respiration, 
#    and ra is a growth efficiency term. Hall & Tank select a moderate growth 
#    efficiency = 0.2 and low growth efficiency = 0.05 to bound their estimate
# 5. Heterotrophic N assimilation can be calculated assuming a respiratory 
#    quotient of 1 and a C:N scaling ratio of 20:1 (both Hall & Tank 2003 and
#    Covino et al 2018 use 20 as ratio)

# Equations -----------------------------------------------------------------

# Autotrophic N assimilation = GPP [gO2/m2d] * Respiratory quotient [g C/g O2] 
#                                  * Autotrophic respiration coefficient 
#                                  * C:N scaling ratio [g N / g C]
# Ua [g N / m2 day ] = GPP * 1 * 0.5 * 1/20 or 1/12

# Heterotrophic respiration = ER - ra * GPP
# HR [g O2 / m2 day] = ER - 0.5 or 0.2 * GPP

# Heterotrophic N assimilation = HR [g O2 / m2 day] 
#                                * Respiratory quotient [g C / g O2]
#                                * C:N scaling ratio [g N / g C]
# Heterotrophic N assimilation = HR * 1 * 1/20

# Assumed constants ----------------------------------------------------------

RespQuot <- 1
AutoRespCoeff <- 0.5
CNscale <- 20

ra.med <- 0.5
ra.low <- 0.2

# Run calculation
stoich.S1S2 <- 
  modelfit.S1S2 %>%
  select(date, GPP, ER) %>%
  mutate(ER = abs(ER),
         Uaut_gNm2day = GPP * RespQuot * AutoRespCoeff / CNscale,
         HR = ER - ra.low * GPP,
         Uhet_gNm2day = HR / CNscale,
         U_gNm2day = Uaut_gNm2day + Uhet_gNm2day)
stoich.desoto <- 
  modelfit.desoto$predictions %>%
  select(date, GPP, ER) %>%
  mutate(ER = abs(ER),
         Uaut_gNm2day = GPP * RespQuot * AutoRespCoeff / CNscale,
         HR = ER - ra.low * GPP,
         Uhet_gNm2day = HR / CNscale,
         U_gNm2day = Uaut_gNm2day + Uhet_gNm2day)

# Save to RData and CSV
saveRDS(stoich.S1S2, file = "./Outputs/StoichUptake_S1S2.RData")
write.csv(stoich.S1S2, file ="./Outputs/StoichUptake_S1S2.CSV", row.names = F)
saveRDS(stoich.desoto, file = "./Outputs/StoichUptake_Desoto.RData")
write.csv(stoich.desoto, file ="./Outputs/StoichUptake_Desoto.CSV", row.names = F)
```

```{r depth.check}
# Check StreamMetabolizer calculated depth against USGS stream gauging data
# at DeSoto gauge

# Pull calculated depth data from stream metabolism model
desoto_calcZ <- modelfit.desoto[["fit"]]@data

# Load in gage height data from USGS sensor at Desoto (06892350)
desoto_gage <- read.csv("./Data/USGSDesoto_GageHeight.csv")
# Convert ft to m
desoto_gage$GageHeight_m <- desoto_gage$GageHeight_ft * 0.3048
# Format DateTime
desoto_gage$DateTime <- lubridate::mdy_hm(desoto_gage$DateTime, 
                                          tz = "America/Chicago")
# Take daily mean of Gage heights
desoto_gage <- 
  desoto_gage %>%
  group_by(date = as.Date(DateTime)) %>%
  summarise(GageHeight_m = mean(GageHeight_m))

# Take daily mean of calculated depth
desoto_calcZ <- 
  desoto_calcZ %>%
  group_by(date) %>%
  summarise(calcdepth_m = mean(depth))

# Merge
desoto_calcZ <- dplyr::full_join(desoto_calcZ, desoto_gage, by = "date")
# Plot
ggpubr::ggscatter(desoto_calcZ, x = "GageHeight_m", y = "calcdepth_m", 
                  add = "reg.line", 
                  add.params = list(color = "red", fill = "lightgray"), 
                  conf.int = TRUE) +
  xlab("Gage height, USGS at DeSoto (m)") +
  ylab("Modeled mean depth at S3, StreamMetabolizer (m)") +
  ggpubr::stat_cor(aes(label = paste(..rr.label.., ..p.label.., sep = "~`,`~")))
# Linear regression
model <- lm(calcdepth_m ~ GageHeight_m, data = desoto_calcZ)
summary(model)
```
