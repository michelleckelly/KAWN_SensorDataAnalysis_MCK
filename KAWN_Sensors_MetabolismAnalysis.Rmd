---
title: "Metabolism Modeling"
author: "Michelle Catherine Kelly"
output:
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: console
---

Copyright (c) 2019 Michelle Catherine Kelly  

License: MIT License  

```{r setup}
# grab streamPULSE pipeline tools
#install.packages("devtools")
#devtools::install_github('streampulse/StreamPULSE', dependencies=TRUE)

# grab latest version of streamMetabolizer
#remotes::install_github('appling/unitted')
#remotes::install_github("USGS-R/streamMetabolizer")

# load packages
library(StreamPULSE)
library(streamMetabolizer)
library(tidyverse)
library(lubridate)
```

```{r metabolism.load}
# Load in cleaned metabolism data from file
# Skip this block if you haven't run the metabolism model, or if you want 
# to re-compute the model
modelfit.S1S2 <- readRDS("./Outputs/MetabResults_TwoStation.RData")
modelfit.desoto <- readRDS("./Outputs/MetabResults_Desoto.RData")
```

```{r singleStation}
# Model stream metabolism from raw data using StreamPULSE database and 
# streamMetabolizer package

# Define the desired model type for streamMetabolizer
model_type <-  "bayes"
# Define the desired modeling framework 
model_name <-  "streamMetabolizer"

# Desoto ------------------------------------------------------
site_code.desoto <- "KS_KANSASR"
desoto_data <- request_data(site_code.desoto)
fitdata.desoto <- prep_metabolism(desoto_data, type = model_type, 
                                  model = model_name, rm_flagged = "Bad Data", 
                                  fillgaps = "interpolation",
                                  estimate_areal_depth = TRUE)
modelfit.desoto <- fit_metabolism(fitdata.desoto)

# Inspect model output
# We have 10 days where ER estimate is positive
# Positive ER estimates range from near 0 to ~5.5, some of these days also
# correspond to negative/near 0 GPP estimates
# Conclude that modeling is wonky for these days, exclude them from analysis

# 2018-02-25
modelfit.desoto$predictions[modelfit.desoto$predictions$date == "2018-02-25",][2:7] <- NA
# 2018-02-26
modelfit.desoto$predictions[modelfit.desoto$predictions$date == "2018-02-26",][2:7] <- NA
# 2018-02-27
modelfit.desoto$predictions[modelfit.desoto$predictions$date == "2018-02-27",][2:7] <- NA
# 2018-02-28
modelfit.desoto$predictions[modelfit.desoto$predictions$date == "2018-02-28",][2:7] <- NA
# 2018-03-01
modelfit.desoto$predictions[modelfit.desoto$predictions$date == "2018-03-01",][2:7] <- NA
# 2018-03-02
modelfit.desoto$predictions[modelfit.desoto$predictions$date == "2018-03-02",][2:7] <- NA
# 2018-03-03
modelfit.desoto$predictions[modelfit.desoto$predictions$date == "2018-03-03",][2:7] <- NA
# 2018-03-10
modelfit.desoto$predictions[modelfit.desoto$predictions$date == "2018-03-10",][2:7] <- NA
# 2018-03-11
modelfit.desoto$predictions[modelfit.desoto$predictions$date == "2018-03-11",][2:7] <- NA

saveRDS(modelfit.desoto, file = "./Outputs/MetabResults_Desoto.RData") 
write.csv(modelfit.desoto$predictions, file = "./Outputs/MetabResults_Desoto.csv")
```

```{r areal.extent}
# Compute gas exchange distance at S3
# From Stream Ecosystems in a Changing Environment: 
#   "One-station techniques measure a reach of stream that scales with the 
#   transport distance of oxygen; a proposed distance is about three times 
#   the transport distance of O2 (ie. 3V/K, Chapra and Di Toro 1991), which 
#   corresponds to 95% O2 turnover in the reach. Spatial variability at the 
#   spatial scale of this distance will bias estimates of one-station 
#   metabolism. For example, a sonde placed in a tailwater below a dam may 
#   measure oxygen processes both in the tailwater and in the upstream lake."

# L = 3 v / K600
# Where L = distance along river at which 95% of oxygen has turned over [m]
#       v = average flow velocity [m / day]
#       K600 = gas exchange coefficient predicted by model [day^-1]
# Citations: Chapra & Del Toro 1991
#            Hall et al. 2012
#            Jones & Stanley Stream Ecosystems in a Changing... pg 158

# Grab K600 values from the metabolism modeling data -------------------------
# StreamMetabolizer uses the 50% condfidence interval (not the mean) GPP and 
# ER measurements, which is why I'm grabbing the 50% confidence interval K600
# measurements
K600.desoto <- 
  modelfit.desoto[["fit"]]@fit[["daily"]] %>%
  select(date, K600_daily_50pct) %>%
  filter(date < ymd("2018-05-01") & date >= ymd("2018-02-01")) 

# Interpolate for velocity using v = Q/A -------------------------------------
# Approximate river cross sectional A using interpolated depth and aerial w
# Grab river width - this is from google maps data, same values used in 
# uptake analysis
w.Eric <- 210.9
w.Steve <- 113.4
w.Desoto <- 138.07
# Calculate water velocity based on modeled depth and discharge
v.S1 <- 
  S1_preproc %>%
  select(datetime, depth, discharge) %>%
  group_by(date = date(datetime)) %>%
  filter(date < ymd("2018-05-01") & date >= ymd("2018-02-01")) %>%
  summarise(depth_m = mean(depth), Q_m3s = mean(discharge), 
            A_m2 = w.Eric*depth_m, v_ms = Q_m3s/A_m2,
            v_mday = v_ms*86400)
v.S2 <- 
  S2_preproc %>%
  select(datetime, depth, discharge) %>%
  group_by(date = date(datetime)) %>%
  filter(date < ymd("2018-05-01") & date >= ymd("2018-02-01")) %>%
  summarise(depth_m = mean(depth), Q_m3s = mean(discharge), 
            A_m2 = w.Steve*depth_m, v_ms = Q_m3s/A_m2,
            v_mday = v_ms*86400)
v.desoto <- 
  modelfit.desoto[["fit"]]@data %>%
  select(date, solar.time, depth, discharge) %>%
  group_by(date) %>%
  filter(date < ymd("2018-05-01") & date >= ymd("2018-02-01")) %>%
  summarise(depth_m = mean(depth), Q_m3s = mean(discharge), 
            A_m2 = w.Desoto*depth_m, v_ms = Q_m3s/A_m2,
            v_mday = v_ms*86400)
# Run oxygen turnover distance calculation
K600.desoto <- 
  full_join(K600.desoto, v.desoto) %>%
  mutate(O2TurnL_m = 3*v_mday/K600_daily_50pct, O2TurnL_km = O2TurnL_m/1000)

# Summary stats --------------------------------------------------------------
v.S1 %>% summarise_all(mean, na.rm = T)
v.S2 %>% summarise_all(mean, na.rm = T)
K600.desoto %>% summarise_all(mean, na.rm = T)

# Save as CSV files ----------------------------------------------------------
saveRDS(v.S1, file = "./Outputs/Velocity_S1.RData")
write.csv(v.S1, file = "./Outputs/Velocity_S1.CSV", row.names = F)

saveRDS(v.S2, file = "./Outputs/Velocity_S2.RData")
write.csv(v.S2, file = "./Outputs/Velocity_S2.CSV", 
          row.names = F)

saveRDS(K600.desoto, file = "./Outputs/OxygenTurnover_Desoto.RData")
write.csv(K600.desoto, file = "./Outputs/OxygenTurnover_Desoto.CSV", 
          row.names = F)
```

```{r twoStation}
########### Load libraries ####################################################
library(mcmc)

########### 1. Load raw data from StreamPulse #################################
## Define time range
start <- "2018-02-01"
end <- "2018-05-01"

## S1 ("Eric")
site_code <- "KS_KANSASREASTLAWRENCE"
S1 <- request_data(site_code, startdate = start, enddate = end)
## S2 ("Steve")
site_code <- "KS_KANSASRFALLLEAF"
S2 <- request_data(site_code, startdate = start, enddate = end)

########### 2. Preprocess data using StreamPULSE functions ####################
# Format data for modeling
S1_preproc <- prep_metabolism(S1, rm_flagged = "Bad Data", 
                                fillgaps = "interpolation",
                                estimate_areal_depth = TRUE)
S2_preproc <- prep_metabolism(S2, rm_flagged = "Bad Data", 
                                fillgaps = "interpolation",
                                estimate_areal_depth = TRUE)

########## 3. Rename columns to match script format ###########################
# First actual DO log for S1 is at 2018-02-25 02:55:16
S1_preproc <- 
  S1_preproc$data %>%
  as_tibble() %>%
  rename(datetime = solar.time, temp = temp.water, oxy = DO.obs) %>%
  filter(datetime >= ymd_hms("2018-02-25 02:55:16", tz = "UTC") &
                             datetime < ymd_hms("2018-04-20 14:40:00", tz = "UTC"))
S2_preproc <- 
  S2_preproc$data %>%
  as_tibble() %>%
  rename(datetime = solar.time, temp = temp.water, oxy = DO.obs) %>%
  filter(datetime >= ymd_hms("2018-02-25 02:55:16", tz = "UTC") &
           datetime < ymd_hms("2018-04-20 14:40:00", tz = "UTC"))

# Remove seconds in time, this way datetimes will be equal
second(S1_preproc$datetime) <- 0
second(S2_preproc$datetime) <- 0
# Add column for site
S1_preproc$`river station` <- "S1"
S2_preproc$`river station` <- "S2"
# Merge dataframes
TS_S1S2 <- full_join(S1_preproc, S2_preproc)

# For identifying time periods with overlapping data
#ggplot(data = TS_S1S2, aes(x = datetime, y = oxy, color = `river station`)) +
#  geom_line() +
#  scale_x_datetime(breaks = "1 week")

# Data availability at both S1 and S2 during times:
# 2018-02-25 02:55:00 to 2018-03-05 05:40:00
# 2018-03-12 16:55:00 to 2018-03-28 00:40:00
# 2018-03-29 00:10:00 to 2018-03-30 10:40:00

# Section data
TS_S1S2_a <- 
  TS_S1S2 %>%
  filter(datetime >= ymd_hms("2018-02-25 02:55:00", tz = "UTC") &
           datetime <= ymd_hms("2018-03-05 05:40:00", tz = "UTC"))
TS_S1S2_b <- 
  TS_S1S2 %>%
  filter(datetime >= ymd_hms("2018-03-12 16:55:00", tz = "UTC") &
           datetime <= ymd_hms("2018-03-28 00:40:00", tz = "UTC"))
TS_S1S2_c <- 
  TS_S1S2 %>%
  filter(datetime >= ymd_hms("2018-03-29 00:10:00", tz = "UTC") &
           datetime <= ymd_hms("2018-03-30 10:40:00", tz = "UTC"))

# Any remaining NA gaps?
sum(is.na(TS_S1S2_a))
sum(is.na(TS_S1S2_b))
sum(is.na(TS_S1S2_c))

############# 4. Estimate Travel time #########################################
# Travel time = time for a parcel of H2O to travel from S1 to S2

# Variables needed:
# Mean H2O velocity at S1 (where v = Q/A, see equations in areal.extent chunk) 
# Mean H2O velocity at S2
# Distance between S1 and S2 = 5.13 km = 5130 m

# Load in calculated water velocity, area
S1_v <- readRDS(file = "./Outputs/Velocity_S1.RData")
S2_v <- readRDS(file = "./Outputs/Velocity_S2.RData")

# Filter for dates of interest, join dataframes, at each date compute the mean 
# of velocity at S1 and S2
v_S1S2 <- 
  left_join(S1_v %>% filter(date >= date("2018-02-25")),
          S2_v %>% filter(date >= date("2018-02-25")), 
          by = "date", suffix = c(".S1", ".S2")) %>%
  select(date, v_ms.S1, v_ms.S2)
# Compute the mean v of S1 and S2 on each date
v_S1S2$v_ms.mean <- apply(v_S1S2[c("v_ms.S1","v_ms.S2")], MARGIN = 1, 
                          FUN = mean)

# Section into time blocks
# 2018-02-25 02:55:00 to 2018-03-05 05:40:00
# 2018-03-12 16:55:00 to 2018-03-28 00:40:00
v_S1S2_a <- 
  v_S1S2 %>%
  filter(date >= date("2018-02-25") & date <= date("2018-03-05"))
v_S1S2_b <- 
  v_S1S2 %>%
  filter(date >= date("2018-03-12") & date <= date("2018-03-28"))

# Compute overall mean velocity at S1 and S2, and the mean of the daily means
v_S1S2_a.mean <-
  v_S1S2_a %>%
  summarise(mean(v_ms.S1, na.rm = T), 
            mean(v_ms.S2, na.rm = T), 
            mean(v_ms.mean, na.rm = T),
            sd(v_ms.S1, na.rm = T), 
            sd(v_ms.S2, na.rm = T), 
            sd(v_ms.mean, na.rm = T))

v_S1S2_b.mean <-
  v_S1S2_b %>%
  summarise(mean(v_ms.S1, na.rm = T), 
            mean(v_ms.S2, na.rm = T), 
            mean(v_ms.mean, na.rm = T),
            sd(v_ms.S1, na.rm = T), 
            sd(v_ms.S2, na.rm = T), 
            sd(v_ms.mean, na.rm = T))

# Travel time [s] = distance [m] / velocity [m/s]
# Convert from sec to hrs
# Convert from sec to days
tt_S1S2_a <- tibble(date = v_S1S2_a$date,
                    tt_sec = 5130 / v_S1S2_a$v_ms.mean,
                    tt_hours = tt_sec / 3600,
                    tt_days = tt_sec / 86400)
tt_S1S2_b <- tibble(date = v_S1S2_b$date,
                    tt_sec = 5130 / v_S1S2_b$v_ms.mean,
                    tt_hours = tt_sec / 3600,
                    tt_days = tt_sec / 86400)

# Compute mean and standard deviation
tt_S1S2_a.mean <- 
  tt_S1S2_a %>%
  summarise(mean(tt_days, na.rm = T),
            sd(tt_days, na.rm = T))
tt_S1S2_b.mean <- 
  tt_S1S2_b %>%
  summarise(mean(tt_days, na.rm = T),
            sd(tt_days, na.rm = T))

# overall mean tt, for normalizing U
tt_S1S1_overall.mean <- 
  v_S1S2 %>%
  filter(date >= date("2018-02-25") & date <= date("2018-03-28")) %>%
  summarise(v_ms.mean = mean(v_ms.mean, na.rm = T)) %>%
  mutate(tt_sec.mean = 5130 / v_ms.mean,
         tt_hours.mean = tt_sec.mean / 3600,
         tt_days.mean = tt_sec.mean / 86400)

############# 5. Pull z #########################################
z_S1S2 <- 
  left_join(S1_v %>% filter(date >= date("2018-02-25")),
          S2_v %>% filter(date >= date("2018-02-25")), 
          by = "date", suffix = c(".S1", ".S2")) %>%
  select(date, depth_m.S1, depth_m.S2)

# Section into time blocks
# 2018-02-25 02:55:00 to 2018-03-05 05:40:00
# 2018-03-12 16:55:00 to 2018-03-28 00:40:00
z_S1S2_a <- 
  z_S1S2 %>%
  filter(date >= date("2018-02-25") & date <= date("2018-03-05"))
z_S1S2_b <- 
  z_S1S2 %>%
  filter(date >= date("2018-03-12") & date <= date("2018-03-28"))

# Compute the mean of S1 and S2 on each date
z_S1S2_a$z.mean <- apply(z_S1S2_a[c("depth_m.S1","depth_m.S2")], 
                       MARGIN = 1, FUN = mean)
z_S1S2_b$z.mean <- apply(z_S1S2_b[c("depth_m.S1","depth_m.S2")], 
                       MARGIN = 1, FUN = mean)

# Compute overall mean at S1 and S2, and the mean of the daily means
z_S1S2_a.mean <- 
  z_S1S2_a %>% 
  summarise(mean(depth_m.S1, na.rm = T), 
            mean(depth_m.S2, na.rm = T), 
            mean(z.mean, na.rm = T),
            sd(depth_m.S1, na.rm = T), 
            sd(depth_m.S2, na.rm = T), 
            sd(z.mean, na.rm = T))
z_S1S2_b.mean <- 
  z_S1S2_b %>% 
  summarise(mean(depth_m.S1, na.rm = T), 
            mean(depth_m.S2, na.rm = T), 
            mean(z.mean, na.rm = T),
            sd(depth_m.S1, na.rm = T), 
            sd(depth_m.S2, na.rm = T), 
            sd(z.mean, na.rm = T))

############## The below script is adapted from Hall et al 2016 ###############
# Load chron package
library(chron)

# Function to convert time column to chron object
timeConvert <- function(data){
  # Mutate time column
  data <- 
    data %>%
    mutate(date = date(datetime), 
           time = paste(hour(datetime), minute(datetime), second(datetime), 
                        sep = ":"))
  # Convert to chron object
  dtime <- chron(dates = as.character(data$date), 
               times = as.character(data$time),
               format = c(dates = "y-m-d", times = "h:m:s"))
  return(dtime)
}

TS_S1S2$dtime <- timeConvert(TS_S1S2)
TS_S1S2_a$dtime <- timeConvert(TS_S1S2_a)
TS_S1S2_b$dtime <- timeConvert(TS_S1S2_b)
TS_S1S2_c$dtime <- timeConvert(TS_S1S2_c)

############################ Internal Functions ###############################
# Function to estimate K at temperature 'temp'from K600.  Wanninkhof 1992.
Kcor <- function (temp, K600) {
	K600 / (600/(1800.6-(temp*120.1)+(3.7818*temp^2)-(0.047608*temp^3)))^-0.5
}

########### tspost FUNCTION: Calculate posterior probablility of model ########
# This function calculates the posterior probability of the model given 
# parameters
# Called within twostationpostsum
#
# ARGUMENTS:
# MET          Dataframe name of cleaned raw two station data (ex. "TS_S1S2")
# tempup        Temperature data from upstream station [deg C]
# tempdown      Temperature data from downstream station [deg C]
# oxyup         Oxygen data from upstream station [mg-O2/L]
# oxydown       Oxygen data from downstream station [mg-O2/L]
# Light         [any light unit]
# tt            travel time [days]
# Kmean   
# Ksd
tspost <- function(MET, tempup, tempdown, oxyup, oxydown, light, tt, z, osat, Kmean, Ksd){
	# Assign the paramters we solve for to easy to understand values
	GPP <- MET[1]
	ER <- MET[2]
	K <- MET[3]
	# Always model the log of variance so that one does not get a 
	# negative standard deviation
	sigma <- exp(MET[4]) 
	
	lag <- as.numeric(round(tt/0.0104166667))
	
	metab <- vector(mode = "numeric", length = length(oxyup)) #create empty vector

  # Below is equation 4 in the paper, solving for downstream O2 at at each 5 
	# min interval. It references other functions:  Kcor converts K600 to KO2 
	# for a given temperature. 
  for (i in 1:length(oxyup)){
    metab[i] <- (oxyup[i] + ((GPP/z)*(sum(light[i:(i+lag)])/sum(light))) + 
                   ER*tt/z + 
                   (Kcor(tempup[i],Kmean))*tt*(osat[i] - 
                                             oxyup[i] + 
                                             osat[i])/2) / 
      (1+ Kcor(tempup[i],Kmean)*tt/2) 
    }
	
	# likelhood is below.  dnorm caculates the probablity density of a normal 
	# distribution, note log.
	loglik <- sum(dnorm(oxydown, metab, sigma, log=TRUE))
	
	# Priors, note wide distributions for GPP and ER
	prior <- (dnorm(GPP, mean=5, sd=10, log=TRUE)) + 
	  (dnorm(ER, mean=-5, sd=10, log=TRUE)) + 
	  (dnorm(K, mean=Kmean, sd=Ksd, log=TRUE))
	
	loglik + prior 
}

##### O2TimeSeries FUNCTION: Return modeled oxygen time series from ##########
###################### median GPP, ER estimates ##############################
# ARGUMENTS:
# GPP
# ER
# O2data    Dataframe of cleaned raw two station data (ex. "TS_S1S2")
# Kmean
# z
# tt
# upName        Character name of upstream station (ex. "S1")
# downName      Character name of downstream station (ex. "S2")
O2TimeSeries <- function(GPP, ER, O2data, Kmean, z, tt, upName, downName) {
  # Ungroup O2data
  O2data <- O2data %>% ungroup()
  
	#number of 5 min readings bewteen up and down probe corresponding 
  # to travel time tt
	lag <- as.numeric(round(tt/0.0104166667))
	
	# trim the ends of the oxy and temp data by the lag so that oxydown[1] 
	# is the value that is the travel time later than oxy up. The below 
	# calls are designed to work with our data structure.
	
	# Seperate data into upstream and downstream sections
  updata <- O2data[O2data$`river station` == upName,]
  downdata <- O2data[O2data$`river station` == downName,]
	
	tempup <- updata$temp[1:as.numeric(length(updata$temp)-lag)] # trim the end by the lag
	tempdown <- downdata$temp[(1+lag):length(downdata$temp)]
	
	oxyup <- updata$oxy[1:as.numeric(length(updata$temp)-lag)]
	# define osat 
	osat <- updata$DO.sat[1:as.numeric(length(updata$temp)-lag)]
	oxydown <- downdata$oxy[(1+lag):length(downdata$temp)]
	
	timeup <- updata$dtime[1:(length(updata$temp)-lag)]
	timedown <- downdata$dtime[(1+lag):length(downdata$temp)]
	
	light <- downdata$light
  
	# Initialize an empty vector
	modeledO2 <- numeric(length(oxyup))
	# Calculate metabolism at each timestep
	for (i in 1:length(oxyup)) {
	  modeledO2[i] <- (oxyup[i] + ((GPP/z)*(sum(light[i:(i+lag)]) / sum(light))) +
	                 ER*tt/z + 
	                 (Kcor(tempup[i],Kmean))*tt*(osat[i] - 
	                                               oxyup[i] + 
	                                           osat[i])/2) / 
	    (1 + Kcor(tempup[i],Kmean)*tt/2) 
	}
	
	# Convert time from chron to posixct
	timeup <- as.POSIXlt(timeup)
	timedown <- as.POSIXlt(timedown)
	
  oxymodel <- data.frame(timeup, timedown, oxydown, oxyup, modeledO2)
  return(oxymodel)
}

################# Two station modeling function ###############################
########### twostationpostsum FUNCTION: Calculate MCMC for data ###############
# This function prepares the data for any given river, runs the MCMC and 
# returns posterior distributions
# ARGUMENTS:
# data          Dataframe name of cleaned raw two station data (ex. "TS_S1S2")
# upName        Character name of upstream station (ex. "S1")
# downName      Character name of downstream station (ex. "S2")
# start         First guess at GPP, ER ranges
# z             average river depth [m]
# tt            travel time [days]
# Kmean         mean K for study period
# Ksd           standard deviation of K
# nbatch        number of MCMC trials
# scale         

twostationpostsum <- function(O2data, upName, downName, start, z, tt, Kmean, Ksd, 
                              nbatch, scale) {
  # Add date column
  O2data <- O2data %>% group_by(date = date(datetime))
  # Create list of unique dates in data
  dateList <- unique(O2data$date)
  
  # Initialize empty vectors to store results from modeling
  #date <- vector(length = length(unique(data$date)))
  GPP <- vector(mode = "numeric", length = length(unique(O2data$date)))
  GPP.lower <- vector(mode = "numeric", length = length(unique(O2data$date)))
  GPP.upper <- vector(mode = "numeric", length = length(unique(O2data$date)))
  ER <- vector(mode = "numeric", length = length(unique(O2data$date)))
  ER.lower <- vector(mode = "numeric", length = length(unique(O2data$date)))
  ER.upper <- vector(mode = "numeric", length = length(unique(O2data$date)))
  K <- vector(mode = "numeric", length = length(unique(O2data$date)))
  K.lower <- vector(mode = "numeric", length = length(unique(O2data$date)))
  K.upper <- vector(mode = "numeric", length = length(unique(O2data$date)))
  s <- vector(mode = "numeric", length = length(unique(O2data$date)))
  s.lower <- vector(mode = "numeric", length = length(unique(O2data$date)))
  s.upper <- vector(mode = "numeric", length = length(unique(O2data$date)))
  accept <- vector(length = length(unique(O2data$date)))
  
  # For loop iterating through each day of data
  for (i in 1:length(dateList)){
    # Subset data to single day of interest (ith date in date string)
    data <- subset(O2data, date == dateList[i])
    # Seperate data into upstream and downstream sections
    updata <- data[data$`river station` == upName,]
    downdata <- data[data$`river station` == downName,]
    
    # NOTE: In Hall et al, the number below was 0.00347222, which corresponds to:
    # 0.00347222 days = 5 minutes, as their O2 sensors took 5-minute readings.
    # Here, our sensors took 15 minute readings. So:
    # 15 minutes = 0.0104166667 days
    # number of 15 min readings bewteen up and down probe corresponding to travel 
    # time tt
    lag <- as.numeric(round(tt/0.0104166667))
    
    # trim the ends of the oxy and temp data by the lag so that oxydown[1] 
    # is the value that is the travel time later than oxy up.  The below calls 
    # are designed to work with our data structure.
    
    if (length(updata$temp) < lag) {
      # If there is less data during a day than the lag interval, move to next day
      message("ERROR: model not computed for ", dateList[i], " as insufficient observations provided.")
      break
    } else{
      # Else continue
      tempup <- updata$temp[1:as.numeric(length(updata$temp)-lag)] # trim the end by the lag
    	oxyup <- updata$oxy[1:as.numeric(length(updata$temp)-lag)]
    	# define osat 
    	osat <- updata$DO.sat[1:as.numeric(length(updata$temp)-lag)]
    	
    	tempdown <- downdata$temp[(1+lag):length(downdata$temp)]
    	oxydown <- downdata$oxy[(1+lag):length(downdata$temp)]
    	
    	light <- downdata$light
    	
    	# perform MCMC
    	# see documentation on mcmc
    	met.post <- metrop(tspost, initial = start, nbatch = nbatch, scale = scale, 
    	                   tempup = tempup, tempdown = tempdown, oxyup = oxyup, 
    	                   osat = osat,
    	                   oxydown = oxydown,  z = z, light = light, tt = tt, 
    	                   Kmean = Kmean, Ksd = Ksd, debug = TRUE)
    	
    	# Calculate overall estimates for each day
      gppr <- quantile(met.post$batch[(950:nbatch),1], c(0.025, 0.5, 0.975))
      err <- quantile(met.post$batch[(950:nbatch),2], c(0.025, 0.5, 0.975))
      Kr <- quantile(met.post$batch[(950:nbatch),3], c(0.025, 0.5, 0.975))
      sr <- quantile(met.post$batch[(950:nbatch),4], c(0.025, 0.5, 0.975))
      
      # Add results to vectors
      #date[i] <- ymd(unique(data$date)[i])
      GPP[i] <- gppr[2]
      GPP.lower[i] <- gppr[1]
      GPP.upper[i] <- gppr[3]
      ER[i] <- err[2]
      ER.lower[i] <- err[1]
      ER.upper[i] <- err[3]
      K[i] <- Kr[2]
      K.lower[i] <- Kr[1]
      K.upper[i] <- Kr[3]
      s[i] <- sr[2]
      s.lower[i] <- sr[1]
      s.upper[i] <- sr[3]
      accept[i] <- met.post$accept # log likelihood plus priors, should be about 0.2
    }
  }
  # Create dataframe of predicted metabolism values
  pred.metab <- data.frame(date = dateList, GPP, GPP.lower, GPP.upper, ER, ER.lower, 
                            ER.upper, K, K.lower, K.upper, s, s.lower, s.upper)
  
  # Call O2TimeSeries function to return modeled O2 values based on median GPP and ER
  # modeling results
  oxymodel <- O2TimeSeries(GPP = pred.metab$GPP, ER = pred.metab$ER, 
                           O2data = O2data, Kmean = Kmean, z = z, tt = tt, 
                           upName = upName, downName = downName)
  
  # Create output list to return to user
  output <- list(pred.metab = pred.metab, accept = accept, oxymodel = oxymodel)
  
  return(output)
}

##function for nighttime regression.  Reports K600, units of 1/d.  Make sure you
# set the times approprately, i.e. just after dark to whenever deltaO2 reaches 0

##Convert KO2 to K600
K600fromO2 <- function(temp, KO2) {
  ((600/(1800.6-(120.1*temp)+(3.7818*temp^2)-(0.047608*temp^3)))^-0.5)*KO2
}

# nightStart    Time (in HH:MM:SS) that nighttime begins in solar time
# nightEnd      Time (in HH:MM:SS) that nighttime ends in solar time
nightreg <- function(o2file, nightStart, nightEnd, downName) {
  # Section off the data so we're only dealing with the downstream data
  o2file <- o2file[o2file$`river station` == downName,]
  
  ## Trim the daily data so it's only during nighttime
  # Create decimal time column
  o2file$decimalTime <- hour(o2file$datetime) + minute(o2file$datetime)/60
  # Calculate hours since noon, where 0 would = noon, 1 = 1PM, 2 = 2PM... 12 = midnight
  o2file$timeSinceNoon <- o2file$decimalTime - 12
  # Convert nightStart to decimal time since noon
  nightStart <- hms(nightStart)
  nightStart <- (hour(nightStart) + minute(nightStart)/60) - 12
  # Convert nightEnd to decimal time since noon
  nightEnd <- hms(nightEnd)
  nightEnd <- (hour(nightEnd) + minute(nightEnd)/60) - 12
  # Subset nighttime data
  o2night <- o2file[data.table::between(o2file$timeSinceNoon, nightStart, 12) | 
                      data.table::between(o2file$timeSinceNoon, -12, nightEnd),]
  
  # Add column to distinguish night time `days` of data
  # We'll then loop through these `days` to calculate K estimate
  # on each `day`
  o2night <- 
    o2night %>%
    mutate(nightBlock = case_when(timeSinceNoon < 0 ~ date(datetime) - 1,
                                  timeSinceNoon > 0 ~ date(datetime)))
  # Create list of unique nightBlocks
  nightList <- unique(o2night$nightBlock)
  
  # Initialize vectors to store results
  regIntercept <- vector(mode = "numeric", length = length(nightList))
  regSlope <- vector(mode = "numeric", length = length(nightList))
  K600 <- vector(mode = "numeric", length = length(nightList))
  #satdeficit <- vector(mode = "numeric", length = nrow(o2night))
  #deltaoxy <- vector(mode = "numeric", length = nrow(o2night))
  
  # Need some way to group by night blocks
  for (i in 1:length(nightList)) {
    # Subset data to single nightblock of interest
    data <- subset(o2night, nightBlock == nightList[i])
    
    # Create lists of temp, oxy, oxysat for each nighttime block
    temp <- data$temp
    oxy <- data$oxy
    oxysat <- data$DO.sat
    
    # Calculate moving average on oxy data
    oxyf1 <- stats::filter(as.numeric(data$oxy), rep(1/3, 3), sides=2)
    # trim the ends of the oxy data
    oxyf2 <- oxyf1[c(-1,-length(oxyf1))]
    # calculate delO/delt
    deltaO2 <- ((oxyf2[-1] - oxyf2[-length(oxyf2)])/15)*1440 ## CHECK: IS /5 B/C THEY HAVE 5 MIN SAMPLING TIME? this is my hunch, so I've adjusted to /15
    
    # Trim the first two and last one from the oxysat data to match the filter oxy data
    oxysattrim <- oxysat[c(-2:-1, -length(oxyf2))]
    # Calc the dodef
    satdef <- oxysattrim - oxyf2[-1]
    
    # Calculate regression
    nreg <- lm(deltaO2 ~ satdef)
    coeff <- coef(nreg)
    # Convert the slope of the regression to K600 value
    calcK600 <- as.numeric(K600fromO2(mean(temp), coeff[2]))
    
    # Save results to lists
    regIntercept[i] <- as.numeric(coeff[1])
    regSlope[i] <- as.numeric(coeff[2])
    K600[i] <- calcK600
    plot(satdef,deltaO2, main = nightList[i])
    abline(nreg)
  }
  # Create dataframe of daily estimates
  dailyEstimates <- data.frame(date = nightList, intercept = regIntercept, 
                               slope = regSlope, K600 = K600)
  
  # Return daily estimates and mean K600 for period
  out <- list(dailyEstimates = dailyEstimates, meanK600 = mean(dailyEstimates$K600))
  return(out)
}

# Run function
k600_a <- nightreg(o2file = TS_S1S2_a, nightStart = "17:45:00", 
                   nightEnd = "06:30:00", downName = "S2")
# Check through the plots to make sure none of them look weird
# 2018-02-24 looks weird, probably because there's just data starting at 3AM onwards 
# for this date
# Therefore, compute mean K600 without 2018-02-24:
mean(k600_a$dailyEstimates$K600[-1])

k600_b <- nightreg(o2file = TS_S1S2_b, nightStart = "17:45:00", 
                   nightEnd = "06:30:00", downName = "S2")
mean(k600_b$dailyEstimates$K600[-1])

k600_c <- nightreg(o2file = TS_S1S2_c, nightStart = "17:45:00", 
                   nightEnd = "06:30:00", downName = "S2")
k600_c$dailyEstimates$K600[2] # Only 2 days of data here

########### Run 2-station metabolism modeling function #######################
# Using K estimates from nighttime regression

# Time block a, 2018-02-25 thru 2018-03-05
tsout_a <- twostationpostsum(start = c(2.75, -4.75, 2.2, -2.6), 
                             O2data = TS_S1S2_a, 
                             z = as.numeric(z_S1S2_a.mean[3]), 
                             tt = as.numeric(tt_S1S2_a.mean$`mean(tt_days, na.rm = T)`), 
                             Kmean = mean(k600_a$dailyEstimates$K600[-1]), 
                             Ksd = sd(k600_a$dailyEstimates$K600[-1]), 
                             upName = "S1", downName = "S2", 
                             nbatch = 200000, scale = 0.3)
# Scale needs to be tuned so that acceptance rate is around 25%
tsout_a[["accept"]]

tsout_b <-twostationpostsum(start = c(2.75, -4.75, 2.2, -2.6), 
                            O2data = TS_S1S2_b, 
                            z = as.numeric(z_S1S2_b.mean[3]), 
                            tt = as.numeric(tt_S1S2_b.mean$`mean(tt_days, na.rm = T)`), 
                             Kmean = mean(k600_b$dailyEstimates$K600[-1]), 
                             Ksd = sd(k600_b$dailyEstimates$K600[-1]),
                            upName = "S1", downName = "S2", 
                            nbatch = 200000, scale = 0.3)
tsout_b[["accept"]]

# Not enough time steps to run group C
tsout_c <-twostationpostsum(start = c(2.75, -4.75, 2.2, -2.6), 
                            O2data = TS_S1S2_c, 
                            z = as.numeric(z_S1S2.mean[3]), 
                            tt = as.numeric(tt_S1S2[3]), 
                             Kmean = mean(k600_c$dailyEstimates$K600[-1]), 
                             Ksd = sd(k600_c$dailyEstimates$K600[-1]),
                            upName = "S1", downName = "S2", 
                            nbatch = 200000, scale = 0.3)
tsout_c[["accept"]]

#tsout_a <- readRDS(file = "./Outputs/MetabResults_TwoStation_a_nighttimeK.RData")
#tsout_b <- readRDS(file = "./Outputs/MetabResults_TwoStation_b_nighttimeK.RData")

## Merge into one dataframe

# Check modeled oxygen
ggplot(data = tsout_a$oxymodel) +
  geom_line(aes(x = timeup, y = oxyup, color = "S1")) +
  geom_line(aes(x = timedown, y = oxydown, color = "S2")) +
  geom_line(aes(x = timedown, y = modeledO2, color = "Modeled O2"))
# Check modeled GPP and ER
ggplot(data = tsout_a$pred.metab) +
  geom_ribbon(aes(x = date, ymin = GPP.lower, ymax = GPP.upper, 
                  fill = "GPP"), alpha = 0.5) +
  geom_line(aes(x = date, y = GPP, color = "GPP")) +
  geom_line(aes(x = date, y = -ER, color = "ER")) +
  geom_ribbon(aes(x = date, ymin = -ER.lower, ymax = -ER.upper, 
                  fill = "ER"), alpha = 0.5)

ggplot(data = tsout_b$pred.metab) +
  geom_ribbon(aes(x = date, ymin = GPP.lower, ymax = GPP.upper, 
                  fill = "GPP"), alpha = 0.5) +
  geom_line(aes(x = date, y = GPP, color = "GPP")) +
  geom_line(aes(x = date, y = -ER, color = "ER")) +
  geom_ribbon(aes(x = date, ymin = -ER.lower, ymax = -ER.upper, 
                  fill = "ER"), alpha = 0.5)

full_join(tsout_a$pred.metab, tsout_b$pred.metab)

ggplot(data = full_join(tsout_a$pred.metab, tsout_b$pred.metab)) +
  geom_ribbon(aes(x = date, ymin = GPP.lower, ymax = GPP.upper, 
                  fill = "GPP"), alpha = 0.5) +
  geom_line(aes(x = date, y = GPP, color = "GPP")) +
  geom_line(aes(x = date, y = -ER, color = "ER")) +
  geom_ribbon(aes(x = date, ymin = -ER.lower, ymax = -ER.upper, 
                  fill = "ER"), alpha = 0.5)

# Create dataframe of dates based on S3 modeled dates
#dateSeq <- tibble(date = seq(ymd('2018-01-31'), ymd('2018-05-31'), by='1 day'))
# Merge data
modelfit.S1S2 <-full_join(tsout_a$pred.metab, tsout_b$pred.metab)

# Remove dates containing partial data
# 2018-03-05
# 2018-03-12
# 2018-03-28
modelfit.S1S2[2:13][modelfit.S1S2$date == "2018-03-05",] <- NA
modelfit.S1S2[2:13][modelfit.S1S2$date == "2018-03-12",] <- NA
modelfit.S1S2[2:13][modelfit.S1S2$date == "2018-03-28",] <- NA

# Save joined data
saveRDS(modelfit.S1S2, file = "./Outputs/MetabResults_TwoStation.RData")
```

```{r NEP.calculate}
# Calculate Net Ecosystem Production (NEP) from metabolism model output

# Arguments
# model_output    dataframe returned by metabolismModeling
# Returns
# dataframe of date, cumulative NEP, cumulative GPP, and cumulative ER for site

NEP_calcs <- function(model_output){
  NEP_data <- data.frame(date = model_output$date,
                         GPP = model_output$GPP,
                         ER = model_output$ER)
  # to perform cumulative sum correctly, any NA values must be set to 0
  NEP_data[is.na(NEP_data)] <- 0
  # take cumulative sum
  NEP_data <- data.frame(date = NEP_data$date,
                         GPP_sum = cumsum(NEP_data$GPP),
                         ER_sum = cumsum(NEP_data$ER))
  # GPP + ER = NEP
  NEP_data$NEP <- rowSums(NEP_data[, c(2,3)])
  return(NEP_data)
}

# NEP calculations
NEP.S1S2 <- NEP_calcs(modelfit.S1S2)
NEP.desoto <- NEP_calcs(modelfit.desoto$predictions)
```

```{r stoich.scaling}
# Predict areal N uptake rate based on metabolism
# Assumptions: (Hall & Tank 2003)
# 1. C production will drive autotrophic & heterotrophic demand for N (AKA, 
#    assume a respiratory quotient = 1)
# 2. Molar C:N scaling ratio = 20 [g C / g N] (Hall & Tank 2003) OR 
#    Molar C:N scaling ratio = 12 [g C / g N] (Covino et al 2018, citing 
#    steltzer & lamberti 2001)
# 3. Net autotrophic production is 0.5*GPP (Odum 1957, Webster & Myer 1997)
# 4. Heterotrophic respiration can be estimated from ecosystem respiration
#    using the equation HR = ER - ra*GPP where HR is herotrophic respiration, 
#    and ra is a growth efficiency term. Hall & Tank select a moderate growth 
#    efficiency = 0.2 and low growth efficiency = 0.05 to bound their estimate
# 5. Heterotrophic N assimilation can be calculated assuming a respiratory 
#    quotient of 1 and a C:N scaling ratio of 20:1 (both Hall & Tank 2003 and
#    Covino et al 2018 use 20 as ratio)

# Equations -----------------------------------------------------------------

# Autotrophic N assimilation = GPP [gO2/m2d] * Respiratory quotient [g C/g O2] 
#                                  * Autotrophic respiration coefficient 
#                                  * C:N scaling ratio [g N / g C]
# Ua [g N / m2 day ] = GPP * 1 * 0.5 * 1/20 or 1/12

# Heterotrophic respiration = ER - ra * GPP
# HR [g O2 / m2 day] = ER - 0.5 or 0.2 * GPP

# Heterotrophic N assimilation = HR [g O2 / m2 day] 
#                                * Respiratory quotient [g C / g O2]
#                                * C:N scaling ratio [g N / g C]
# Heterotrophic N assimilation = HR * 1 * 1/20

# Assumed constants ----------------------------------------------------------

RespQuot <- 1
AutoRespCoeff <- 0.5
CNscale <- 20

ra.med <- 0.5
ra.low <- 0.2

# Run calculation
stoich.S1S2 <- 
  modelfit.S1S2 %>%
  select(date, GPP, ER) %>%
  mutate(ER = abs(ER),
         Uaut_gNm2day = GPP * RespQuot * AutoRespCoeff / CNscale,
         HR = ER - ra.low * GPP,
         Uhet_gNm2day = HR / CNscale,
         U_gNm2day = Uaut_gNm2day + Uhet_gNm2day)
stoich.desoto <- 
  modelfit.desoto$predictions %>%
  select(date, GPP, ER) %>%
  mutate(ER = abs(ER),
         Uaut_gNm2day = GPP * RespQuot * AutoRespCoeff / CNscale,
         HR = ER - ra.low * GPP,
         Uhet_gNm2day = HR / CNscale,
         U_gNm2day = Uaut_gNm2day + Uhet_gNm2day)

# Save to RData and CSV
saveRDS(stoich.S1S2, file = "./Outputs/StoichUptake_S1S2.RData")
write.csv(stoich.S1S2, file ="./Outputs/StoichUptake_S1S2.CSV", row.names = F)
saveRDS(stoich.desoto, file = "./Outputs/StoichUptake_Desoto.RData")
write.csv(stoich.desoto, file ="./Outputs/StoichUptake_Desoto.CSV", row.names = F)
```

```{r depth.check}
# Check StreamMetabolizer calculated depth against USGS stream gauging data
# at DeSoto gauge

# Pull calculated depth data from stream metabolism model
desoto_calcZ <- modelfit.desoto[["fit"]]@data

# Load in gage height data from USGS sensor at Desoto (06892350)
desoto_gage <- read.csv("./Data/USGSDesoto_GageHeight.csv")
# Convert ft to m
desoto_gage$GageHeight_m <- desoto_gage$GageHeight_ft * 0.3048
# Format DateTime
desoto_gage$DateTime <- lubridate::mdy_hm(desoto_gage$DateTime, 
                                          tz = "America/Chicago")
# Take daily mean of Gage heights
desoto_gage <- 
  desoto_gage %>%
  group_by(date = as.Date(DateTime)) %>%
  summarise(GageHeight_m = mean(GageHeight_m))

# Take daily mean of calculated depth
desoto_calcZ <- 
  desoto_calcZ %>%
  group_by(date) %>%
  summarise(calcdepth_m = mean(depth))

# Merge
desoto_calcZ <- dplyr::full_join(desoto_calcZ, desoto_gage, by = "date")
# Plot
ggpubr::ggscatter(desoto_calcZ, x = "GageHeight_m", y = "calcdepth_m", 
                  add = "reg.line", 
                  add.params = list(color = "red", fill = "lightgray"), 
                  conf.int = TRUE) +
  xlab("Gage height, USGS at DeSoto (m)") +
  ylab("Modeled mean depth at S3, StreamMetabolizer (m)") +
  ggpubr::stat_cor(aes(label = paste(..rr.label.., ..p.label.., sep = "~`,`~")))
# Linear regression
model <- lm(calcdepth_m ~ GageHeight_m, data = desoto_calcZ)
summary(model)
```
